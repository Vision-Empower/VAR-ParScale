# PARScale-VAR Phase 2A 技术现状（中文版·平衡版）

⸻

## 一、总体结论
- **成果**：搭建了完备的 H100 实验环境、性能与多样性评测脚本，深入摸清了 VAR 内部架构与批处理边界，为后续工作打下了基础。
- **不足**：真正的「共享骨干 / KV-Cache 累积」尚未打通；并行效率、显存缩放、多样性三项核心指标仍未全部达标。

⸻

## 二、已取得的实质进展

| 模块 | 具体收获 |
|------|----------|
| 基础设施 | 云端脚本自动化、显存/延迟/多样性实时监控、错误超效率检测 (>100% 时报警) |
| 架构理解 | 16 层 AdaLNSelfAttn 结构、mat_qkv 投影路径、VAR 原生 B=batch 支持的优缺点 |
| 测评基线 | 明确了 P=1,2,4 时真实延迟与显存数据；建立了可重复的对比方法 |
| 问题定位 | 找到瓶颈集中在 外层流级 for-loop 和 KV-Cache 未累积 |

⸻

## 三、当前瓶颈与挑战

### 3.1 顺序依赖
- 自回归时间维度天然串行；要想共享计算，必须重构 KV-Cache 传递逻辑。

### 3.2 KV-Cache 集成难度  
- VAR 自带 C++/CUDA 后端缓存策略，Python 层硬插 torch.cat 容易形状错、显存不涨。

### 3.3 测量精准度
- 少一次 cuda.synchronize() 就会出现 200%+「效率幻觉」，需警惕。

### 3.4 多样性
- VAR 默认生成高度确定；若无参数或输入扰动，流间余弦相似度≈1。

⸻

## 四、真实性能（H100，max_steps=64）

```
P=1  Lat ≈400 ms   PeakMem 2.0 GB   Diversity 0.000
P=2  Lat ≈800 ms   PeakMem 4.0 GB   Diversity ≈0.02
P=4  Lat ≈1600 ms  PeakMem 8.0 GB   Diversity ≈0.03
```

**说明**：目前仍是"并发调用"而非"共享骨干"。显存与延迟近似线性增长。

⸻

## 五、下一阶段务实路线

### 5.1 彻底删除流级 for-loop

```python
# 目标：一次调用处理 P·B
out = self.autoregressive_infer_batch(tokens_PB, ...)
```

### 5.2 先让第 0 层 KV-Cache 真正递增
- 每步打印 `kv[0]['k'].shape`，确认第三维从 1 → T 递增。

### 5.3 显存验证
- 看到 P=2 ≥1.6×、P=4 ≥2.5× 的显存涨幅，才算真正共享。

### 5.4 Nsight Profiling
- Self-Attention Kernel 次数应为 layer × step；多出就是假并行。

### 5.5 多样性放到最后
- 先解决并行与缓存，再讨论 top-k/top-p 扰动或输入变换。

⸻

## 六、价值与展望

### 短期价值
已有的测评框架、脚本和架构洞察可直接用于后续优化，也可为其他团队节省重复摸索时间。

### 中期目标
在 P=2 配置下达成：
- Latency 380-430 ms
- PeakMem ≥1.6×
- Diversity ≥0.15

并作为扩展到 P=4、P=8 的基础。

### 长期研究
探索 speculative decoding、Flash/KV Paging 等方案，从算法层面突破自回归并行瓶颈。

⸻

**一句话**：我们已经站在正确的起跑线上，但真正的"共享骨干"这一关还没攻下；务实推进，比任何乐观宣言都更重要。

---
*Phase 2A 技术现状报告 - VAR-ParScale 项目*  
*基础已稳，核心待攻*