#!/usr/bin/env python3
"""
Experiment 3: Direct Comparison (ParScale-VAR P=2 vs. Baseline VAR)
ç›®æ ‡: ç›´æ¥å¯¹æ¯”ParScale-VAR P=2ä¸åŸºçº¿VARæ€§èƒ½

éªŒè¯å‡è®¾:
1. è´¨é‡å‡è®¾: ParScale-VAR FIDæ›´ä½, ISæ›´é«˜ (ç›¸ä¼¼å‚æ•°æ•°é‡)
2. æ•ˆç‡å‡è®¾: å»¶è¿Ÿ â‰¤1.2xåŸºçº¿, æ˜¾å­˜ <<2xåŸºçº¿
"""

import torch
import torch.nn as nn
import numpy as np
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import logging
from dataclasses import dataclass
from scipy import stats

@dataclass
class ComparisonConfig:
    """å¯¹æ¯”å®éªŒé…ç½®"""
    num_evaluation_samples: int = 10000
    batch_size: int = 20
    num_repeated_runs: int = 3  # ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯
    significance_threshold: float = 0.05
    efficiency_latency_target: float = 1.2  # 1.2xåŸºçº¿å»¶è¿Ÿç›®æ ‡
    
class StatisticalComparison:
    """ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•"""
    
    @staticmethod
    def paired_t_test(baseline_metrics: List[float], 
                     parscale_metrics: List[float], 
                     alpha: float = 0.05) -> Dict:
        """é…å¯¹tæ£€éªŒ"""
        
        # æ‰§è¡Œé…å¯¹tæ£€éªŒ
        t_stat, p_value = stats.ttest_rel(parscale_metrics, baseline_metrics)
        
        # è®¡ç®—æ•ˆåº”å¤§å° (Cohen's d)
        diff = np.array(parscale_metrics) - np.array(baseline_metrics)
        cohens_d = np.mean(diff) / np.std(diff)
        
        result = {
            't_statistic': t_stat,
            'p_value': p_value,
            'is_significant': p_value < alpha,
            'cohens_d': cohens_d,
            'effect_size': 'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large',
            'mean_baseline': np.mean(baseline_metrics),
            'mean_parscale': np.mean(parscale_metrics),
            'improvement_pct': (np.mean(parscale_metrics) - np.mean(baseline_metrics)) / np.mean(baseline_metrics) * 100
        }
        
        return result

class DirectComparisonExperiment:
    """ç›´æ¥å¯¹æ¯”å®éªŒå®æ–½"""
    
    def __init__(self, config: ComparisonConfig = None):
        self.config = config or ComparisonConfig()
        self.setup_logging()
        
        # ç»“æœå­˜å‚¨
        self.baseline_results = []
        self.parscale_results = []
        self.comparison_summary = {}
        
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - EXP3-Comparison - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('exp3_comparison.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def compare_model_parameters(self, baseline_model, parscale_model) -> Dict:
        """å¯¹æ¯”æ¨¡å‹å‚æ•°æ•°é‡"""
        
        baseline_params = sum(p.numel() for p in baseline_model.parameters())
        parscale_params = sum(p.numel() for p in parscale_model.parameters())
        
        # ParScaleé¢å¤–å‚æ•° (å˜æ¢å¤´ + èšåˆå¤´)
        shared_backbone_params = sum(p.numel() for p in parscale_model.shared_var_backbone.parameters())
        additional_params = parscale_params - shared_backbone_params
        
        param_comparison = {
            'baseline_parameters': baseline_params,
            'parscale_total_parameters': parscale_params,
            'shared_backbone_parameters': shared_backbone_params,
            'additional_parameters': additional_params,
            'parameter_ratio': parscale_params / baseline_params,
            'additional_param_percentage': (additional_params / baseline_params) * 100
        }
        
        self.logger.info(f"ğŸ“Š å‚æ•°å¯¹æ¯”: {param_comparison}")
        
        # éªŒè¯å‚æ•°æ•ˆç‡
        if param_comparison['parameter_ratio'] > 1.3:  # è¶…è¿‡30%å¢åŠ 
            self.logger.warning("âš ï¸  ParScaleå‚æ•°å¢åŠ è¶…è¿‡30%, å¯èƒ½å½±å“å…¬å¹³å¯¹æ¯”")
            
        return param_comparison
        
    def measure_model_performance(self, model, model_name: str, 
                                device='cuda', is_parscale=False) -> Dict:
        """æµ‹é‡å•ä¸ªæ¨¡å‹æ€§èƒ½"""
        
        self.logger.info(f"ğŸ“Š æµ‹é‡ {model_name} æ€§èƒ½...")
        
        model.eval()
        
        # æ€§èƒ½æŒ‡æ ‡æ”¶é›†
        generated_images = []
        inference_times = []
        memory_usages = []
        
        num_samples = self.config.num_evaluation_samples
        batch_size = self.config.batch_size
        
        with torch.no_grad():
            for batch_idx in range(0, num_samples, batch_size):
                current_batch_size = min(batch_size, num_samples - batch_idx)
                
                # æ˜¾å­˜åŸºçº¿
                torch.cuda.empty_cache()
                memory_before = torch.cuda.memory_allocated(device)
                
                # æ¨ç†æ—¶é—´æµ‹é‡
                start_time = time.time()
                torch.cuda.synchronize()
                
                try:
                    if is_parscale:
                        # ParScaleæ¨ç† (åŒ…å«å¤šæµå¤„ç†)
                        generated_tokens = model.autoregressive_infer_cfg(
                            B=current_batch_size,
                            label_B=None,
                            cfg=1.0,
                            top_k=900,
                            top_p=0.95
                        )
                    else:
                        # åŸºçº¿VARæ¨ç†
                        generated_tokens = model.autoregressive_infer_cfg(
                            B=current_batch_size,
                            label_B=None,
                            cfg=1.0,
                            top_k=900,
                            top_p=0.95
                        )
                        
                    # TODO: è§£ç ä¸ºå›¾åƒ (éœ€è¦VQVAE)
                    # generated_imgs = vae_model.decode(generated_tokens)
                    
                except Exception as e:
                    self.logger.error(f"{model_name} ç”Ÿæˆå¤±è´¥: {e}")
                    continue
                
                torch.cuda.synchronize()
                end_time = time.time()
                
                memory_after = torch.cuda.memory_allocated(device)
                
                # è®°å½•æ€§èƒ½
                batch_inference_time = (end_time - start_time) / current_batch_size
                inference_times.append(batch_inference_time)
                memory_usages.append(memory_after - memory_before)
                
                # generated_images.extend(generated_imgs.cpu())
                
                if batch_idx % 1000 == 0:
                    avg_time = np.mean(inference_times) * 1000
                    self.logger.info(f"{model_name} è¿›åº¦: {batch_idx + current_batch_size}/{num_samples}, "
                                   f"å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        # TODO: å®ç°çœŸå®FID/ISè®¡ç®—
        fid_score = self.calculate_fid_placeholder()
        is_score = self.calculate_is_placeholder()
        
        results = {
            'model_name': model_name,
            'fid_score': fid_score,
            'inception_score': is_score,
            'avg_inference_time_ms': np.mean(inference_times) * 1000,
            'inference_time_std': np.std(inference_times) * 1000,
            'peak_memory_gb': max(memory_usages) / (1024**3),
            'avg_memory_gb': np.mean(memory_usages) / (1024**3),
            'samples_evaluated': len(inference_times) * batch_size
        }
        
        self.logger.info(f"âœ… {model_name} ç»“æœ: {results}")
        return results
        
    def calculate_fid_placeholder(self) -> float:
        """FIDè®¡ç®—å ä½ç¬¦"""
        # åŸºçº¿: 2.1, ParScaleç›®æ ‡: 1.8-1.9 (æ”¹å–„)
        return np.random.uniform(1.8, 2.2)
        
    def calculate_is_placeholder(self) -> float:
        """ISè®¡ç®—å ä½ç¬¦"""
        # åŸºçº¿: 342, ParScaleç›®æ ‡: 350-360 (æ”¹å–„)
        return np.random.uniform(340, 365)
        
    def run_repeated_comparison(self, baseline_model, parscale_model) -> Dict:
        """æ‰§è¡Œå¤šæ¬¡é‡å¤å¯¹æ¯” (ç»Ÿè®¡æ˜¾è‘—æ€§)"""
        
        self.logger.info(f"ğŸ”„ æ‰§è¡Œ {self.config.num_repeated_runs} æ¬¡é‡å¤å¯¹æ¯”...")
        
        baseline_fids = []
        baseline_iss = []
        baseline_latencies = []
        
        parscale_fids = []
        parscale_iss = []
        parscale_latencies = []
        
        # å¤šæ¬¡è¿è¡Œ
        for run_idx in range(self.config.num_repeated_runs):
            self.logger.info(f"ğŸ“Š æ‰§è¡Œç¬¬ {run_idx + 1}/{self.config.num_repeated_runs} æ¬¡è¿è¡Œ...")
            
            # åŸºçº¿VAR
            baseline_result = self.measure_model_performance(
                baseline_model, f"Baseline_Run_{run_idx}", is_parscale=False
            )
            baseline_fids.append(baseline_result['fid_score'])
            baseline_iss.append(baseline_result['inception_score'])
            baseline_latencies.append(baseline_result['avg_inference_time_ms'])
            
            # ParScale-VAR
            parscale_result = self.measure_model_performance(
                parscale_model, f"ParScale_P2_Run_{run_idx}", is_parscale=True
            )
            parscale_fids.append(parscale_result['fid_score'])
            parscale_iss.append(parscale_result['inception_score'])
            parscale_latencies.append(parscale_result['avg_inference_time_ms'])
            
        # ç»Ÿè®¡åˆ†æ
        statistical_results = {
            'fid_comparison': StatisticalComparison.paired_t_test(baseline_fids, parscale_fids),
            'is_comparison': StatisticalComparison.paired_t_test(baseline_iss, parscale_iss),
            'latency_comparison': StatisticalComparison.paired_t_test(baseline_latencies, parscale_latencies)
        }
        
        return statistical_results
        
    def evaluate_hypotheses(self, statistical_results: Dict, param_comparison: Dict) -> Dict:
        """è¯„ä¼°éªŒè¯å‡è®¾"""
        
        self.logger.info("ğŸ¯ è¯„ä¼°æ ¸å¿ƒå‡è®¾...")
        
        # å‡è®¾1: è´¨é‡æ”¹å–„ (FIDé™ä½, ISæå‡)
        fid_improved = (statistical_results['fid_comparison']['improvement_pct'] < 0 and 
                       statistical_results['fid_comparison']['is_significant'])
        
        is_improved = (statistical_results['is_comparison']['improvement_pct'] > 0 and
                      statistical_results['is_comparison']['is_significant'])
        
        quality_hypothesis_met = fid_improved or is_improved
        
        # å‡è®¾2: æ•ˆç‡å¯æ¥å— (å»¶è¿Ÿ â‰¤1.2x)
        latency_ratio = (statistical_results['latency_comparison']['mean_parscale'] / 
                        statistical_results['latency_comparison']['mean_baseline'])
        
        efficiency_hypothesis_met = latency_ratio <= self.config.efficiency_latency_target
        
        # å‚æ•°æ•ˆç‡
        parameter_efficiency_good = param_comparison['parameter_ratio'] <= 1.5  # å‚æ•°å¢åŠ <50%
        
        hypothesis_evaluation = {
            'quality_hypothesis': {
                'met': quality_hypothesis_met,
                'fid_improved': fid_improved,
                'is_improved': is_improved,
                'fid_improvement_pct': statistical_results['fid_comparison']['improvement_pct'],
                'is_improvement_pct': statistical_results['is_comparison']['improvement_pct']
            },
            'efficiency_hypothesis': {
                'met': efficiency_hypothesis_met,
                'latency_ratio': latency_ratio,
                'target_ratio': self.config.efficiency_latency_target,
                'latency_improvement_pct': statistical_results['latency_comparison']['improvement_pct']
            },
            'parameter_efficiency': {
                'acceptable': parameter_efficiency_good,
                'parameter_ratio': param_comparison['parameter_ratio'],
                'additional_params_pct': param_comparison['additional_param_percentage']
            },
            'overall_success': quality_hypothesis_met and efficiency_hypothesis_met and parameter_efficiency_good
        }
        
        self.logger.info(f"ğŸ¯ å‡è®¾è¯„ä¼°ç»“æœ: {hypothesis_evaluation}")
        
        return hypothesis_evaluation
        
    def generate_decision_recommendation(self, hypothesis_results: Dict) -> str:
        """ç”Ÿæˆå†³ç­–å»ºè®®"""
        
        if hypothesis_results['overall_success']:
            decision = "âœ… PROCEED: ParScale-VARæ¦‚å¿µéªŒè¯æˆåŠŸ!"
            reasoning = [
                "è´¨é‡å‡è®¾å¾—åˆ°éªŒè¯",
                "æ•ˆç‡å‡è®¾æ»¡è¶³ç›®æ ‡", 
                "å‚æ•°æ•ˆç‡å¯æ¥å—",
                "å»ºè®®è¿›è¡ŒPhase 2è¯¦ç»†æ¶ˆèç ”ç©¶"
            ]
        else:
            failed_aspects = []
            if not hypothesis_results['quality_hypothesis']['met']:
                failed_aspects.append("è´¨é‡æ”¹å–„ä¸æ˜¾è‘—")
            if not hypothesis_results['efficiency_hypothesis']['met']:
                failed_aspects.append("æ•ˆç‡æˆæœ¬è¿‡é«˜")
            if not hypothesis_results['parameter_efficiency']['acceptable']:
                failed_aspects.append("å‚æ•°å¼€é”€è¿‡å¤§")
                
            decision = "âš ï¸  INVESTIGATE: éœ€è¦è¯Šæ–­å’Œè°ƒä¼˜"
            reasoning = [
                f"å¤±è´¥æ–¹é¢: {', '.join(failed_aspects)}",
                "å»ºè®®æ£€æŸ¥å®ç°ç»†èŠ‚",
                "è°ƒä¼˜è¶…å‚æ•° (Î», æ¸©åº¦è°ƒåº¦)",
                "è€ƒè™‘æ”¹è¿›å˜æ¢T_iè®¾è®¡",
                "æ¶ˆèç ”ç©¶å˜ä¸ºè¯Šæ–­å·¥å…·"
            ]
            
        recommendation = {
            'decision': decision,
            'reasoning': reasoning,
            'next_steps': reasoning[-2:] if not hypothesis_results['overall_success'] else ["Phase 2æ¶ˆèç ”ç©¶", "æ‰©å±•På€¼å®éªŒ"]
        }
        
        self.logger.info(f"ğŸ¯ å†³ç­–å»ºè®®: {recommendation}")
        
        return recommendation
        
    def save_comparison_report(self, all_results: Dict, output_dir: Path):
        """ä¿å­˜å®Œæ•´å¯¹æ¯”æŠ¥å‘Š"""
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        comprehensive_report = {
            'experiment': 'ParScale-VAR P=2 vs Baseline VAR Direct Comparison',
            'configuration': {
                'num_evaluation_samples': self.config.num_evaluation_samples,
                'num_repeated_runs': self.config.num_repeated_runs,
                'significance_threshold': self.config.significance_threshold
            },
            'results': all_results,
            'timestamp': time.time(),
            'device': 'H100 80GB'
        }
        
        report_path = output_dir / 'exp3_comparison_report.json'
        with open(report_path, 'w') as f:
            json.dump(comprehensive_report, f, indent=2)
            
        self.logger.info(f"ğŸ“ å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """æ‰§è¡Œç›´æ¥å¯¹æ¯”å®éªŒ"""
    print("ğŸš€ å¼€å§‹ Experiment 3: ParScale-VAR P=2 vs åŸºçº¿VARç›´æ¥å¯¹æ¯”")
    print("ğŸ¯ éªŒè¯å‡è®¾: è´¨é‡æ”¹å–„ + æ•ˆç‡å¯æ¥å—")
    
    experiment = DirectComparisonExperiment()
    
    # TODO: åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    print("âš ï¸  TODO: åŠ è½½åŸºçº¿VARæ¨¡å‹")
    print("âš ï¸  TODO: åŠ è½½è®­ç»ƒå¥½çš„ParScale-VAR P=2æ¨¡å‹")
    print("âš ï¸  TODO: æ‰§è¡Œå¤šæ¬¡é‡å¤å¯¹æ¯”")
    print("âš ï¸  TODO: ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ")
    
    # baseline_model = load_baseline_var()
    # parscale_model = load_parscale_var_p2()
    
    # param_comparison = experiment.compare_model_parameters(baseline_model, parscale_model)
    # statistical_results = experiment.run_repeated_comparison(baseline_model, parscale_model)
    # hypothesis_results = experiment.evaluate_hypotheses(statistical_results, param_comparison)
    # decision = experiment.generate_decision_recommendation(hypothesis_results)
    
    print("âš–ï¸  ç›´æ¥å¯¹æ¯”å®éªŒæ¡†æ¶å·²å°±ç»ª")

if __name__ == "__main__":
    main()