#!/usr/bin/env python3
"""
Decision Framework: ParScale-VARæ¦‚å¿µéªŒè¯å†³ç­–ç‚¹
åŸºäºå®éªŒ3ç»“æœç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨

å†³ç­–é€»è¾‘:
âœ… æˆåŠŸ â†’ Phase 2è¯¦ç»†æ¶ˆèç ”ç©¶
âš ï¸  é—®é¢˜ â†’ è¯Šæ–­è°ƒä¼˜ â†’ é‡æ–°è¯„ä¼°
âŒ å¤±è´¥ â†’ é‡æ–°è®¾è®¡æ¦‚å¿µ
"""

import json
from pathlib import Path
from typing import Dict, List
from dataclasses import dataclass
from enum import Enum

class DecisionOutcome(Enum):
    """å†³ç­–ç»“æœæšä¸¾"""
    PROCEED_TO_ABLATION = "proceed_to_phase2_ablation"
    INVESTIGATE_AND_TUNE = "investigate_and_tune"
    REDESIGN_CONCEPT = "redesign_concept"

@dataclass
class DecisionCriteria:
    """å†³ç­–æ ‡å‡†"""
    # è´¨é‡æ”¹å–„æ ‡å‡†
    min_fid_improvement_pct: float = -5.0    # FIDè‡³å°‘æ”¹å–„5%
    min_is_improvement_pct: float = 2.0      # ISè‡³å°‘æ”¹å–„2%
    
    # æ•ˆç‡æ ‡å‡†
    max_latency_ratio: float = 1.2           # å»¶è¿Ÿæœ€å¤š1.2x
    max_memory_ratio: float = 1.8            # æ˜¾å­˜æœ€å¤š1.8x (è¿œå°äº2x)
    
    # å‚æ•°æ•ˆç‡æ ‡å‡†
    max_parameter_ratio: float = 1.5         # å‚æ•°æœ€å¤šå¢åŠ 50%
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§
    significance_threshold: float = 0.05

class ParScaleVARDecisionFramework:
    """ParScale-VARå†³ç­–æ¡†æ¶"""
    
    def __init__(self, criteria: DecisionCriteria = None):
        self.criteria = criteria or DecisionCriteria()
        
    def evaluate_quality_hypothesis(self, comparison_results: Dict) -> Dict:
        """è¯„ä¼°è´¨é‡å‡è®¾"""
        
        fid_results = comparison_results.get('fid_comparison', {})
        is_results = comparison_results.get('is_comparison', {})
        
        # FIDæ”¹å–„ (é™ä½)
        fid_improved = (
            fid_results.get('improvement_pct', 0) <= self.criteria.min_fid_improvement_pct and
            fid_results.get('is_significant', False)
        )
        
        # ISæ”¹å–„ (æå‡)
        is_improved = (
            is_results.get('improvement_pct', 0) >= self.criteria.min_is_improvement_pct and
            is_results.get('is_significant', False)
        )
        
        quality_score = 0
        if fid_improved: quality_score += 1
        if is_improved: quality_score += 1
        
        quality_evaluation = {
            'fid_improved': fid_improved,
            'is_improved': is_improved,
            'quality_score': quality_score,  # 0-2åˆ†
            'quality_hypothesis_met': quality_score >= 1,  # è‡³å°‘ä¸€ä¸ªæŒ‡æ ‡æ”¹å–„
            'details': {
                'fid_improvement_pct': fid_results.get('improvement_pct', 0),
                'fid_significant': fid_results.get('is_significant', False),
                'is_improvement_pct': is_results.get('improvement_pct', 0),
                'is_significant': is_results.get('is_significant', False)
            }
        }
        
        return quality_evaluation
        
    def evaluate_efficiency_hypothesis(self, comparison_results: Dict) -> Dict:
        """è¯„ä¼°æ•ˆç‡å‡è®¾"""
        
        latency_results = comparison_results.get('latency_comparison', {})
        
        # å»¶è¿Ÿæ¯”ç‡
        latency_ratio = (latency_results.get('mean_parscale', 0) / 
                        max(latency_results.get('mean_baseline', 1), 1))
        
        latency_acceptable = latency_ratio <= self.criteria.max_latency_ratio
        
        # TODO: æ·»åŠ æ˜¾å­˜æ¯”ç‡è¯„ä¼° (éœ€è¦ä»å®éªŒç»“æœè·å–)
        memory_ratio = 1.3  # å ä½ç¬¦
        memory_acceptable = memory_ratio <= self.criteria.max_memory_ratio
        
        efficiency_score = 0
        if latency_acceptable: efficiency_score += 1
        if memory_acceptable: efficiency_score += 1
        
        efficiency_evaluation = {
            'latency_acceptable': latency_acceptable,
            'memory_acceptable': memory_acceptable,
            'efficiency_score': efficiency_score,  # 0-2åˆ†
            'efficiency_hypothesis_met': efficiency_score >= 1,
            'details': {
                'latency_ratio': latency_ratio,
                'latency_target': self.criteria.max_latency_ratio,
                'memory_ratio': memory_ratio,
                'memory_target': self.criteria.max_memory_ratio
            }
        }
        
        return efficiency_evaluation
        
    def evaluate_parameter_efficiency(self, param_comparison: Dict) -> Dict:
        """è¯„ä¼°å‚æ•°æ•ˆç‡"""
        
        param_ratio = param_comparison.get('parameter_ratio', 1.0)
        param_acceptable = param_ratio <= self.criteria.max_parameter_ratio
        
        additional_param_pct = param_comparison.get('additional_param_percentage', 0)
        
        param_evaluation = {
            'parameter_ratio_acceptable': param_acceptable,
            'parameter_efficiency_met': param_acceptable,
            'details': {
                'parameter_ratio': param_ratio,
                'parameter_target': self.criteria.max_parameter_ratio,
                'additional_param_pct': additional_param_pct
            }
        }
        
        return param_evaluation
        
    def make_decision(self, experiment_results: Dict) -> Dict:
        """åšå‡ºæœ€ç»ˆå†³ç­–"""
        
        # è§£æå®éªŒç»“æœ
        comparison_results = experiment_results.get('statistical_results', {})
        param_comparison = experiment_results.get('parameter_comparison', {})
        
        # è¯„ä¼°å„é¡¹å‡è®¾
        quality_eval = self.evaluate_quality_hypothesis(comparison_results)
        efficiency_eval = self.evaluate_efficiency_hypothesis(comparison_results)
        param_eval = self.evaluate_parameter_efficiency(param_comparison)
        
        # è®¡ç®—æ€»åˆ†
        total_score = (quality_eval['quality_score'] + 
                      efficiency_eval['efficiency_score'] + 
                      (2 if param_eval['parameter_efficiency_met'] else 0))
        
        # å†³ç­–é€»è¾‘
        if total_score >= 5:  # æ»¡åˆ†6åˆ†ï¼Œè‡³å°‘5åˆ†
            decision = DecisionOutcome.PROCEED_TO_ABLATION
            confidence = "high"
            rationale = "ParScale-VARæ¦‚å¿µéªŒè¯æˆåŠŸï¼Œæ‰€æœ‰æ ¸å¿ƒå‡è®¾å¾—åˆ°éªŒè¯"
            
        elif total_score >= 3:  # ä¸­ç­‰åˆ†æ•°
            decision = DecisionOutcome.INVESTIGATE_AND_TUNE
            confidence = "medium"
            rationale = "ParScale-VARæ˜¾ç¤ºæ½œåŠ›ä½†éœ€è¦è°ƒä¼˜"
            
        else:  # ä½åˆ†
            decision = DecisionOutcome.REDESIGN_CONCEPT
            confidence = "low"
            rationale = "å½“å‰è®¾è®¡æœªè¾¾åˆ°é¢„æœŸï¼Œéœ€è¦é‡æ–°æ€è€ƒæ¦‚å¿µ"
            
        # ç”Ÿæˆå…·ä½“è¡ŒåŠ¨å»ºè®®
        action_plan = self.generate_action_plan(decision, quality_eval, efficiency_eval, param_eval)
        
        decision_result = {
            'decision': decision.value,
            'confidence': confidence,
            'total_score': total_score,
            'max_score': 6,
            'rationale': rationale,
            'evaluations': {
                'quality': quality_eval,
                'efficiency': efficiency_eval,
                'parameter_efficiency': param_eval
            },
            'action_plan': action_plan
        }
        
        return decision_result
        
    def generate_action_plan(self, decision: DecisionOutcome, 
                           quality_eval: Dict, efficiency_eval: Dict, param_eval: Dict) -> List[str]:
        """ç”Ÿæˆå…·ä½“è¡ŒåŠ¨è®¡åˆ’"""
        
        if decision == DecisionOutcome.PROCEED_TO_ABLATION:
            return [
                "ğŸš€ ç«‹å³å¯åŠ¨Phase 2è¯¦ç»†æ¶ˆèç ”ç©¶",
                "ğŸ“Š ä¼˜å…ˆPå€¼æ‰©å±•å®éªŒ (P=4, P=8)",
                "ğŸ” åˆ†æTransformå±‚T_iæœ‰æ•ˆæ€§",
                "ğŸ“ˆ ç ”ç©¶å¤šæ ·æ€§æ­£åˆ™åŒ–æœ€ä¼˜Î»å€¼",
                "âš™ï¸ æ¢ç´¢Token-wise vs Globalèšåˆç­–ç•¥",
                "ğŸ“ å‡†å¤‡é«˜è´¨é‡ç ”ç©¶è®ºæ–‡"
            ]
            
        elif decision == DecisionOutcome.INVESTIGATE_AND_TUNE:
            issues = []
            solutions = []
            
            if not quality_eval['quality_hypothesis_met']:
                issues.append("è´¨é‡æ”¹å–„ä¸è¶³")
                solutions.extend([
                    "ğŸ”§ è°ƒæ•´å¤šæ ·æ€§æŸå¤±æƒé‡Î» (å½“å‰å¯èƒ½è¿‡å°/è¿‡å¤§)",
                    "ğŸ”„ å®éªŒä¸åŒT_iå˜æ¢ (rotation, scaling, color)",
                    "ğŸŒ¡ï¸ ä¼˜åŒ–æ¸©åº¦è°ƒåº¦ç­–ç•¥",
                    "ğŸ“Š å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§"
                ])
                
            if not efficiency_eval['efficiency_hypothesis_met']:
                issues.append("æ•ˆç‡æˆæœ¬è¿‡é«˜")
                solutions.extend([
                    "âš¡ ä¼˜åŒ–å…±äº«KVç¼“å­˜å®ç°",
                    "ğŸ”€ ç®€åŒ–Token-wiseèšåˆè®¡ç®—",
                    "ğŸ’¾ å®ç°æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–",
                    "â±ï¸ åˆ†ææ¨ç†ç“¶é¢ˆå¹¶ä¼˜åŒ–"
                ])
                
            if not param_eval['parameter_efficiency_met']:
                issues.append("å‚æ•°å¼€é”€è¿‡å¤§")
                solutions.extend([
                    "ğŸ“‰ å‡å°‘èšåˆå¤´å‚æ•°",
                    "ğŸ”— æ¢ç´¢å‚æ•°å…±äº«ç­–ç•¥",
                    "âœ‚ï¸ ç²¾ç®€å˜æ¢å±‚è®¾è®¡"
                ])
                
            action_plan = [f"âš ï¸  è¯†åˆ«é—®é¢˜: {', '.join(issues)}"] + solutions + [
                "ğŸ”„ é‡æ–°æ‰§è¡Œå®éªŒ3éªŒè¯æ”¹è¿›æ•ˆæœ",
                "ğŸ“‹ å¦‚æœä»æœªè¾¾æ ‡ï¼Œè€ƒè™‘é‡æ–°è®¾è®¡"
            ]
            
            return action_plan
            
        else:  # REDESIGN_CONCEPT
            return [
                "ğŸ”„ é‡æ–°è¯„ä¼°ParScale-VARæ ¸å¿ƒå‡è®¾",
                "ğŸ“š æ·±å…¥ç ”ç©¶VARæ¶æ„é™åˆ¶",
                "ğŸ’¡ æ¢ç´¢æ›¿ä»£å¹¶è¡ŒåŒ–ç­–ç•¥",
                "ğŸ¯ è€ƒè™‘ä¸åŒçš„å¤šæ ·æ€§æœºåˆ¶",
                "ğŸ”¬ æˆ–è½¬å‘å…¶ä»–æ¨¡å‹æ¶æ„èåˆ",
                "â¸ï¸ æš‚åœå½“å‰å®ç°ï¼Œé‡æ–°è®¾è®¡æ¦‚å¿µ"
            ]
            
    def save_decision_report(self, decision_result: Dict, output_dir: Path):
        """ä¿å­˜å†³ç­–æŠ¥å‘Š"""
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        decision_report = {
            'decision_framework': 'ParScale-VAR Concept Verification',
            'criteria_used': {
                'min_fid_improvement_pct': self.criteria.min_fid_improvement_pct,
                'min_is_improvement_pct': self.criteria.min_is_improvement_pct,
                'max_latency_ratio': self.criteria.max_latency_ratio,
                'max_memory_ratio': self.criteria.max_memory_ratio,
                'max_parameter_ratio': self.criteria.max_parameter_ratio
            },
            'decision_result': decision_result,
            'timestamp': time.time()
        }
        
        report_path = output_dir / 'decision_framework_report.json'
        with open(report_path, 'w') as f:
            json.dump(decision_report, f, indent=2)
            
        # ç”Ÿæˆå¯è¯»æ€§æ€»ç»“
        summary_path = output_dir / 'decision_summary.txt'
        with open(summary_path, 'w') as f:
            f.write(f"ParScale-VARæ¦‚å¿µéªŒè¯å†³ç­–æŠ¥å‘Š\n")
            f.write(f"=" * 50 + "\n\n")
            f.write(f"ğŸ¯ å†³ç­–ç»“æœ: {decision_result['decision']}\n")
            f.write(f"ğŸšï¸ ç½®ä¿¡åº¦: {decision_result['confidence']}\n")
            f.write(f"ğŸ“Š æ€»åˆ†: {decision_result['total_score']}/{decision_result['max_score']}\n\n")
            f.write(f"ğŸ“ å†³ç­–ç†ç”±:\n{decision_result['rationale']}\n\n")
            f.write(f"ğŸ“‹ è¡ŒåŠ¨è®¡åˆ’:\n")
            for i, action in enumerate(decision_result['action_plan'], 1):
                f.write(f"{i}. {action}\n")
                
        print(f"ğŸ“ å†³ç­–æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print(f"ğŸ“„ å†³ç­–æ€»ç»“å·²ä¿å­˜: {summary_path}")

def main():
    """å†³ç­–æ¡†æ¶ä¸»ç¨‹åº"""
    print("ğŸ¯ ParScale-VARæ¦‚å¿µéªŒè¯å†³ç­–æ¡†æ¶")
    print("ğŸ“Š åŸºäºå®éªŒ3ç»“æœè¿›è¡Œæ™ºèƒ½å†³ç­–")
    
    # TODO: åŠ è½½å®éªŒ3ç»“æœ
    print("âš ï¸  TODO: åŠ è½½å®éªŒ3å¯¹æ¯”ç»“æœ")
    print("âš ï¸  TODO: åº”ç”¨å†³ç­–æ¡†æ¶")
    print("âš ï¸  TODO: ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’")
    
    # ç¤ºä¾‹ç”¨æ³•:
    # decision_framework = ParScaleVARDecisionFramework()
    # experiment3_results = load_experiment3_results()
    # decision = decision_framework.make_decision(experiment3_results)
    # decision_framework.save_decision_report(decision, Path('/Users/peter/VAR-ParScale/results'))
    
    print("ğŸ¯ å†³ç­–æ¡†æ¶å·²å°±ç»ª")

if __name__ == "__main__":
    main()