#!/bin/bash
# ä¿®å¤CUDAç»´åº¦é”™è¯¯ï¼Œå®ç°æ­£ç¡®çš„KV cacheæ‹¼æ¥
set -e

SSH_CMD="ssh -p 11292 root@inst-gw.cloudexe.tech"
echo "ğŸš€ ä¿®å¤CUDAç»´åº¦é”™è¯¯ï¼Œå®ç°æ­£ç¡®çš„KV cacheæ‹¼æ¥"
echo "="*60

$SSH_CMD << 'EOF'
cd /root/VAR

# åˆ›å»ºä¿®å¤CUDAé”™è¯¯çš„ç‰ˆæœ¬
cat > fix_cuda_error.py << 'SCRIPT_EOF'
import torch, time, os, sys, torch.nn.functional as F
os.chdir("/root/VAR"); sys.path.append("/root/VAR")
from models import build_vae_var

# ---------- build models ----------
dev = "cuda"
vae, var = build_vae_var(V=4096,Cvae=32,ch=160,share_quant_resi=4,
                         device=dev,patch_nums=(1,2,3,4,5,6,8,10,13,16),
                         num_classes=1000,depth=16,shared_aln=False)
vae.cuda().eval(); var.cuda().eval()

# ---------- ä¿®å¤çš„attentionæ‰¹å¤„ç†è¡¥ä¸ ----------
import types

def _patch_all_attention_for_batch(model):
    """ä¿®å¤ç‰ˆæœ¬çš„æ‰¹å¤„ç†attentionè¡¥ä¸"""
    for lid, blk in enumerate(model.blocks):
        attn = blk.attn
        if getattr(attn, "_patched", False):
            continue
            
        nh = attn.num_heads
        hdim = attn.head_dim
        
        def batched_attn_forward(self, x, cache):
            # x: [P*B, 1, C]
            B, L, C = x.shape
            if lid == 0:  # åªåœ¨ç¬¬ä¸€å±‚æ‰“å°è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ”¥ Layer {lid} processing batch: {x.shape}")
            
            qkv = self.mat_qkv(x).view(B, L, 3, nh, hdim).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]  # each: [B, nh, L, hdim]
            
            # å®‰å…¨çš„KV cacheç´¯ç§¯
            if "k" in cache and "v" in cache:
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                cached_k, cached_v = cache["k"], cache["v"]
                if lid == 0:
                    print(f"   Prev cache k: {cached_k.shape}, current k: {k.shape}")
                
                # æ£€æŸ¥batchç»´åº¦æ˜¯å¦åŒ¹é…
                if cached_k.size(0) == k.size(0):
                    k = torch.cat([cached_k, k], dim=2)  # concat on sequence length dim
                    v = torch.cat([cached_v, v], dim=2)
                    if lid == 0:
                        print(f"   After concat k: {k.shape}")
                else:
                    # å¦‚æœbatchç»´åº¦ä¸åŒ¹é…ï¼Œé‡æ–°åˆå§‹åŒ–
                    if lid == 0:
                        print(f"   Batch dim mismatch, reinit cache")
            else:
                if lid == 0:
                    print(f"   Initial k: {k.shape}")
            
            # æ›´æ–°cache
            cache["k"], cache["v"] = k, v
            
            # scaled dot-product attention
            scale = hdim ** -0.5
            attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale
            attn_probs = F.softmax(attn_scores, dim=-1)
            out = torch.matmul(attn_probs, v)  # [B, nh, L, hdim]
            
            out = out.transpose(1, 2).reshape(B, L, C)
            return self.proj(out)
        
        attn.forward = types.MethodType(batched_attn_forward, attn)
        attn._patched = True
        
        # ä¿®è¡¥block forwardæ”¯æŒcache
        def block_forward_with_cache(self, x, cache):
            shortcut = x
            x = self.attn(x, cache)
            x = shortcut + self.drop_path(x)
            shortcut = x  
            x = self.ffn(x)
            x = shortcut + self.drop_path(x)
            return x
        
        blk.forward = types.MethodType(block_forward_with_cache, blk)

_patch_all_attention_for_batch(var)

# ---------- ç®€åŒ–çš„å®‰å…¨batchæ¨ç† ----------
def encode_tokens(self, tokens):
    """åˆå§‹åŒ–KV cache"""
    print(f"ğŸš€ Encoding initial tokens: {tokens.shape}")
    # ä¸ºæ¯ä¸€å±‚åˆå§‹åŒ–ç‹¬ç«‹çš„cache
    self._kv_cache = [{} for _ in self.blocks]
    return self.class_emb(tokens)

def forward_step_batch(self, x):
    """å•æ­¥å‰å‘ä¼ æ’­ï¼Œå®‰å…¨ç‰ˆæœ¬"""
    B, L, C = x.shape
    print(f"ğŸ“¦ Forward step input: {x.shape}")
    
    for layer_idx, block in enumerate(self.blocks):
        try:
            x = block(x, cache=self._kv_cache[layer_idx])
        except Exception as e:
            print(f"âŒ Error in layer {layer_idx}: {e}")
            # é‡ç½®è¿™ä¸€å±‚çš„cacheå¹¶é‡è¯•
            self._kv_cache[layer_idx] = {}
            x = block(x, cache=self._kv_cache[layer_idx])
    
    # ä½¿ç”¨VARçš„headè¿›è¡Œè¾“å‡ºæŠ•å½±
    logits = self.head(x)  # [P*B, L, vocab_size]
    print(f"ğŸ“¤ Forward step output: {logits.shape}")
    return logits

@torch.no_grad()
def safe_batch_autoregressive_infer(self, init_tokens, max_steps=16):
    """å®‰å…¨çš„æ‰¹é‡è‡ªå›å½’æ¨ç†ï¼Œè¾ƒçŸ­åºåˆ—"""
    B_all = init_tokens.size(0)
    print(f"ğŸ¯ Starting SAFE batch inference with B_all={B_all}")
    
    # åˆå§‹åŒ–
    x = encode_tokens(self, init_tokens[:, :1])  # [P*B, 1, C]
    generated = [init_tokens[:, :1]]
    
    # è¾ƒçŸ­çš„ç”Ÿæˆå¾ªç¯ä»¥é¿å…å†…å­˜é—®é¢˜
    for step in range(1, max_steps):
        print(f"â° Step {step}/{max_steps}")
        
        try:
            # å•æ­¥å‰å‘ä¼ æ’­
            logits = forward_step_batch(self, x).squeeze(1)  # [P*B, vocab_size]
            
            # ç®€å•é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            next_tokens = torch.multinomial(probs, 1)  # [P*B, 1]
            
            generated.append(next_tokens)
            x = self.class_emb(next_tokens)
            
        except Exception as e:
            print(f"âŒ Error at step {step}: {e}")
            print(f"ğŸ›‘ Stopping early due to error")
            break
    
    print(f"âœ… Batch inference complete!")
    return torch.cat(generated, dim=1)

# ç»‘å®šæ–¹æ³•åˆ°VARå®ä¾‹
var.encode_tokens = types.MethodType(encode_tokens, var)
var.forward_step_batch = types.MethodType(forward_step_batch, var)
var.safe_batch_autoregressive_infer = types.MethodType(safe_batch_autoregressive_infer, var)

# ---------- å®‰å…¨æµ‹è¯• ----------
def run_safe_test(P):
    print(f"\n{'='*50}")
    print(f"ğŸ§ª Testing SAFE batch processing with P={P}")
    print(f"{'='*50}")
    
    # åˆ›å»ºç®€å•çš„åˆå§‹token
    tokens = torch.full((P, 1), 42, device=dev, dtype=torch.long)
    print(f"ğŸ“ Initial tokens: {tokens.flatten().tolist()}")
    
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.time()
    
    try:
        # ä½¿ç”¨è¾ƒçŸ­çš„åºåˆ—é¿å…å†…å­˜çˆ†ç‚¸
        outputs = var.safe_batch_autoregressive_infer(tokens, max_steps=8)
        success = True
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        outputs = None
        success = False
    
    torch.cuda.synchronize()
    
    latency = (time.time() - start_time) * 1000
    peak_mem = torch.cuda.max_memory_allocated() / 1e9
    
    if success and outputs is not None:
        print(f"ğŸ“Š Generated outputs shape: {outputs.shape}")
        print(f"ğŸ“Š Sample outputs:")
        for i in range(min(P, 2)):  # åªæ˜¾ç¤ºå‰2ä¸ªstream
            print(f"   Stream {i}: {outputs[i].cpu().tolist()}")
        
        # è®¡ç®—diversity
        diversity = 0.0
        if P > 1:
            similarities = []
            for i in range(P):
                for j in range(i + 1, P):
                    sim = F.cosine_similarity(
                        outputs[i].float(), outputs[j].float(), dim=0
                    ).item()
                    similarities.append(abs(sim))
            diversity = 1.0 - (sum(similarities) / len(similarities))
    else:
        diversity = 0.0
    
    print(f"P={P:<2}  Lat {latency:>6.1f}ms  PeakMem {peak_mem:.2f}GB  Diversity {diversity:.3f}")
    return success

print("ğŸš€ Running SAFE batch tests...")
print("Goal: Verify KV cache accumulation without CUDA errors")

success_count = 0
for P in (1, 2):  # å…ˆæµ‹è¯•P=1,2
    if run_safe_test(P):
        success_count += 1
    else:
        print(f"âŒ P={P} failed, stopping tests")
        break

if success_count == 2:
    print("\nâœ… P=1,2 successful! Testing P=4...")
    run_safe_test(4)
else:
    print(f"\nâš ï¸ Only {success_count}/2 tests passed")
SCRIPT_EOF

echo "ğŸš€ æ‰§è¡Œå®‰å…¨æ‰¹å¤„ç†æµ‹è¯•..."
python3 fix_cuda_error.py
EOF

echo "âœ… å®‰å…¨æ‰¹å¤„ç†æµ‹è¯•æ‰§è¡Œå®Œæˆ!"