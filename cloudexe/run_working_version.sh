#!/bin/bash
# åŸºäºVARå®é™…ç»“æ„çš„å·¥ä½œç‰ˆæœ¬
set -e

SSH_CMD="ssh -p 11292 root@inst-gw.cloudexe.tech"
echo "ğŸš€ åŸºäºVARå®é™…ç»“æ„çš„å·¥ä½œç‰ˆæœ¬"
echo "="*60

$SSH_CMD << 'EOF'
cd /root/VAR

# æ£€æŸ¥VARæ¨¡å‹çš„å®Œæ•´ç»“æ„å¹¶åˆ›å»ºå·¥ä½œç‰ˆæœ¬
cat > test_working_version.py << 'SCRIPT_EOF'
import torch, time, os, sys, torch.nn.functional as F
os.chdir("/root/VAR"); sys.path.append("/root/VAR")
from models import build_vae_var

# ---------- build models ----------
dev = "cuda"
vae, var = build_vae_var(V=4096,Cvae=32,ch=160,share_quant_resi=4,
                         device=dev,patch_nums=(1,2,3,4,5,6,8,10,13,16),
                         num_classes=1000,depth=16,shared_aln=False)
vae.cuda().eval(); var.cuda().eval()

print("VAR model attributes:")
var_attrs = [attr for attr in dir(var) if not attr.startswith('_')]
print(f"VAR attributes: {var_attrs}")

# æŸ¥æ‰¾æœ€ç»ˆå±‚normalizationå’Œè¾“å‡ºæŠ•å½±
if hasattr(var, 'head'):
    print(f"VAR has head: {type(var.head)}")
if hasattr(var, 'norm'):
    print(f"VAR has norm: {type(var.norm)}")
if hasattr(var, 'final_ln'):
    print(f"VAR has final_ln: {type(var.final_ln)}")

# ---------- ä½¿ç”¨VARç°æœ‰æ–¹æ³•ä½†å¹¶è¡ŒåŒ–è¾“å…¥ ----------
@torch.no_grad()
def parallel_infer(model, P, max_steps=64):
    """ä½¿ç”¨VARç°æœ‰æ–¹æ³•çš„å¹¶è¡Œæ¨ç†"""
    # åˆ›å»ºPä¸ªéšæœºèµ·å§‹token
    init_tokens = torch.randint(0, 1000, (P, 1), device=dev)
    
    # å°†Pä¸ªtokenåˆå¹¶ä¸ºbatchè¿›è¡Œå¤„ç†
    batch_input = init_tokens.view(-1, 1)  # [P, 1]
    
    # ä½¿ç”¨VARçš„ç°æœ‰æ¨ç†ï¼Œä½†ä¼ å…¥batch
    try:
        # å°è¯•ä½¿ç”¨batch input
        outputs = model.autoregressive_infer_cfg(
            B=P, label_B=None, cfg=1.0, top_p=0.95, top_k=900
        )
        return outputs
    except Exception as e:
        print(f"Batch inference failed: {e}")
        # å›é€€åˆ°é€ä¸ªæ¨ç†
        outputs = []
        for i in range(P):
            out = model.autoregressive_infer_cfg(
                B=1, label_B=None, cfg=1.0, top_p=0.95-i*0.01, top_k=900-i*50
            )
            outputs.append(out)
        return outputs

# ---------- run tests for P = 1/2/4 ----------
def run_test(P):
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.time()
    outputs = parallel_infer(var, P, max_steps=64)
    torch.cuda.synchronize()
    
    latency = (time.time() - start_time) * 1000
    peak_mem = torch.cuda.max_memory_allocated() / 1e9
    
    # è®¡ç®—diversity
    diversity = 0.0
    if P > 1 and isinstance(outputs, list):
        similarities = []
        for i in range(P):
            for j in range(i + 1, P):
                sim = F.cosine_similarity(
                    outputs[i].flatten().float(), outputs[j].flatten().float(), dim=0
                ).item()
                similarities.append(abs(sim))
        diversity = 1.0 - (sum(similarities) / len(similarities))
    elif P > 1 and isinstance(outputs, torch.Tensor) and outputs.size(0) == P:
        # å¦‚æœæˆåŠŸæ‰¹å¤„ç†
        similarities = []
        for i in range(P):
            for j in range(i + 1, P):
                sim = F.cosine_similarity(
                    outputs[i].flatten().float(), outputs[j].flatten().float(), dim=0
                ).item()
                similarities.append(abs(sim))
        diversity = 1.0 - (sum(similarities) / len(similarities))
    
    print(f"P={P:<2}  Lat {latency:>6.1f}ms  PeakMem {peak_mem:.2f}GB  Diversity {diversity:.3f}")

print("ğŸš€ Running working version tests...")
for P in (1, 2, 4):
    run_test(P)
SCRIPT_EOF

echo "ğŸš€ æ‰§è¡Œå·¥ä½œç‰ˆæœ¬æµ‹è¯•..."
python3 test_working_version.py
EOF

echo "âœ… å·¥ä½œç‰ˆæœ¬æµ‹è¯•æ‰§è¡Œå®Œæˆ!"