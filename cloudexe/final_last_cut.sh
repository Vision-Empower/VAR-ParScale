#!/bin/bash
# æœ€åä¸€åˆ€ï¼šåˆ é™¤for-loopï¼Œå®ç°çœŸæ­£çš„KV cacheç´¯ç§¯
set -e

SSH_CMD="ssh -p 11292 root@inst-gw.cloudexe.tech"
echo "ğŸ”ª æœ€åä¸€åˆ€ï¼šåˆ é™¤for-loopï¼Œå®ç°çœŸæ­£çš„KV cacheç´¯ç§¯"
echo "="*60

$SSH_CMD << 'EOF'
cd /root/VAR

# å®ç°æœ€åä¸€åˆ€ï¼šçœŸæ­£çš„å•æ¬¡batchè°ƒç”¨
cat > final_last_cut.py << 'SCRIPT_EOF'
import torch, time, os, sys, torch.nn.functional as F
os.chdir("/root/VAR"); sys.path.append("/root/VAR")
from models import build_vae_var

# ---------- build models ----------
dev = "cuda"
vae, var = build_vae_var(V=4096,Cvae=32,ch=160,share_quant_resi=4,
                         device=dev,patch_nums=(1,2,3,4,5,6,8,10,13,16),
                         num_classes=1000,depth=16,shared_aln=False)
vae.cuda().eval(); var.cuda().eval()

print("âœ… VAR model loaded")

# ---------- ç¬¬ä¸€æ­¥ï¼šæœ€å°KV cacheè°ƒè¯•è¡¥ä¸ ----------
import types

def _patch_attention_for_kv_debug(model, debug_layers=[0, 1]):
    """å…ˆåªpatchå‰ä¸¤å±‚ç”¨äºè°ƒè¯•KV cacheç´¯ç§¯"""
    for layer_idx in debug_layers:
        if layer_idx >= len(model.blocks):
            continue
            
        block = model.blocks[layer_idx]
        attn = block.attn
        
        if getattr(attn, "_kv_patched", False):
            continue
            
        num_heads = attn.num_heads
        head_dim = attn.head_dim
        
        def kv_accumulating_forward(self, x, kv_cache):
            # x: [B_all, 1, C]
            B_all, seq_len, C = x.shape
            
            # QKV projection
            qkv = self.mat_qkv(x)  # [B_all, 1, 3*C]
            qkv = qkv.view(B_all, seq_len, 3, num_heads, head_dim)
            qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B_all, num_heads, 1, head_dim]
            q, k, v = qkv[0], qkv[1], qkv[2]
            
            # KV cacheç´¯ç§¯ - è¿™æ˜¯å…³é”®ï¼
            layer_key = f"layer_{layer_idx}"
            if layer_key in kv_cache and "k" in kv_cache[layer_key]:
                # æ‹¼æ¥å†å²KV
                prev_k = kv_cache[layer_key]["k"]  # [B_all, num_heads, t-1, head_dim]
                prev_v = kv_cache[layer_key]["v"]  # [B_all, num_heads, t-1, head_dim]
                
                k = torch.cat([prev_k, k], dim=2)  # [B_all, num_heads, t, head_dim]
                v = torch.cat([prev_v, v], dim=2)  # [B_all, num_heads, t, head_dim]
                
                if layer_idx == 0:  # åªåœ¨layer 0æ‰“å°è°ƒè¯•ä¿¡æ¯
                    print(f"   ğŸ”„ Layer {layer_idx}: KV {prev_k.shape} + {qkv[1].shape} â†’ {k.shape}")
            else:
                if layer_idx == 0:
                    print(f"   ğŸ†• Layer {layer_idx}: KVåˆå§‹åŒ– {k.shape}")
            
            # æ›´æ–°cache
            if layer_key not in kv_cache:
                kv_cache[layer_key] = {}
            kv_cache[layer_key]["k"] = k
            kv_cache[layer_key]["v"] = v
            
            # Attentionè®¡ç®—
            scale = head_dim ** -0.5
            attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale
            attn_weights = F.softmax(attn_weights, dim=-1)
            out = torch.matmul(attn_weights, v)
            
            # é‡ç»„è¾“å‡º
            out = out.transpose(1, 2).contiguous().view(B_all, seq_len, C)
            return self.proj(out)
        
        # æ›¿æ¢attention forward
        attn.forward = types.MethodType(kv_accumulating_forward, attn)
        attn._kv_patched = True
        
        # ä¿®æ”¹block forwardæ”¯æŒkv_cache
        original_block_forward = block.forward
        def block_forward_with_cache(self, x, kv_cache=None):
            if kv_cache is None:
                return original_block_forward(x)
            
            # åªåœ¨è¢«patchçš„å±‚ä½¿ç”¨æ–°é€»è¾‘
            if layer_idx in debug_layers:
                shortcut = x
                attn_out = self.attn(x, kv_cache)
                x = shortcut + self.drop_path(attn_out)
                
                shortcut = x
                ffn_out = self.ffn(x)
                x = shortcut + self.drop_path(ffn_out)
                
                return x
            else:
                return original_block_forward(x)
        
        block.forward = types.MethodType(block_forward_with_cache, block)

print("ğŸ”§ Applying KV debug patches to layers 0,1...")
_patch_attention_for_kv_debug(var, debug_layers=[0, 1])

# ---------- å®ç°æ ¸å¿ƒæ–¹æ³• ----------
def encode_prompt(self, tokens):
    """ç¼–ç tokenså¹¶åˆå§‹åŒ–KV cache"""
    print(f"ğŸ¯ Encoding prompt: {tokens.shape}")
    
    # æ¸…ç©ºKV cache
    self._kv_cache = {}
    
    # ç¼–ç é¦–ä¸ªtoken
    return self.class_emb(tokens)

def forward_step_single(self, x, step):
    """å•æ­¥å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨ç´¯ç§¯çš„KV cache"""
    B_all, seq_len, C = x.shape
    print(f"ğŸ“¦ Step {step}: input {x.shape}")
    
    # åªé€šè¿‡å‰ä¸¤å±‚ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰
    for layer_idx in [0, 1]:
        if layer_idx < len(self.blocks):
            x = self.blocks[layer_idx](x, kv_cache=self._kv_cache)
    
    # ç®€åŒ–çš„è¾“å‡ºæŠ•å½±ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    logits = self.head(x)
    return logits

@torch.no_grad()
def debug_kv_accumulation(self, tokens, max_steps=5):
    """è°ƒè¯•KV cacheç´¯ç§¯æœºåˆ¶"""
    print(f"ğŸ” DEBUG: KV cache accumulation test")
    
    x = encode_prompt(self, tokens)
    
    for step in range(max_steps):
        print(f"\nâ° Step {step}:")
        _ = forward_step_single(self, x, step)
        
        # æ£€æŸ¥KV cacheçŠ¶æ€
        if "layer_0" in self._kv_cache and "k" in self._kv_cache["layer_0"]:
            k_shape = self._kv_cache["layer_0"]["k"].shape
            print(f"   âœ… Layer 0 KV cache shape: {k_shape}")
            
            # éªŒè¯æ—¶é—´ç»´åº¦å¢é•¿
            expected_t = step + 1
            actual_t = k_shape[2]
            if actual_t == expected_t:
                print(f"   âœ… Time dimension correct: {actual_t}")
            else:
                print(f"   âŒ Time dimension mismatch: expected {expected_t}, got {actual_t}")
        else:
            print(f"   âŒ No KV cache found")
        
        # ä¸ºä¸‹ä¸€æ­¥å‡†å¤‡è¾“å…¥ï¼ˆä½¿ç”¨ç›¸åŒçš„token embeddingï¼‰
        x = self.class_emb(tokens[:, :1])

# ç»‘å®šæ–¹æ³•åˆ°VAR
var.encode_prompt = types.MethodType(encode_prompt, var)
var.forward_step_single = types.MethodType(forward_step_single, var)
var.debug_kv_accumulation = types.MethodType(debug_kv_accumulation, var)

print("\nğŸ§ª ç¬¬ä¸€æ­¥ï¼šKV Cacheç´¯ç§¯è°ƒè¯•")
print("="*40)

# æµ‹è¯•KV cacheç´¯ç§¯
test_tokens = torch.randint(0, 1000, (2, 1), device=dev)
print(f"ğŸ“ Test tokens: {test_tokens.flatten().tolist()}")

var.debug_kv_accumulation(test_tokens, max_steps=5)

print("\n" + "="*60)
print("å¦‚æœçœ‹åˆ°æ—¶é—´ç»´åº¦æ­£ç¡®é€’å¢ï¼Œåˆ™ç»§ç»­å®ç°å®Œæ•´ç‰ˆæœ¬...")

# ---------- ç¬¬äºŒæ­¥ï¼šå®Œæ•´å®ç°ï¼ˆå¦‚æœè°ƒè¯•é€šè¿‡ï¼‰ ----------
def _patch_all_layers_for_batch(model):
    """ä¸ºæ‰€æœ‰16å±‚å®ç°KV cacheæ‰¹å¤„ç†"""
    print("ğŸ”§ Patching all 16 layers for batch processing...")
    
    for layer_idx, block in enumerate(model.blocks):
        attn = block.attn
        
        if getattr(attn, "_full_patched", False):
            continue
            
        num_heads = attn.num_heads
        head_dim = attn.head_dim
        
        def full_kv_forward(self, x, kv_cache):
            B_all, seq_len, C = x.shape
            
            # QKV projection
            qkv = self.mat_qkv(x).view(B_all, seq_len, 3, num_heads, head_dim)
            qkv = qkv.permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]
            
            # KV cacheç´¯ç§¯
            layer_key = f"layer_{layer_idx}"
            if layer_key in kv_cache and "k" in kv_cache[layer_key]:
                prev_k = kv_cache[layer_key]["k"]
                prev_v = kv_cache[layer_key]["v"]
                k = torch.cat([prev_k, k], dim=2)
                v = torch.cat([prev_v, v], dim=2)
            
            # æ›´æ–°cache
            if layer_key not in kv_cache:
                kv_cache[layer_key] = {}
            kv_cache[layer_key]["k"] = k
            kv_cache[layer_key]["v"] = v
            
            # Attention
            scale = head_dim ** -0.5
            attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale
            attn_weights = F.softmax(attn_weights, dim=-1)
            out = torch.matmul(attn_weights, v)
            
            out = out.transpose(1, 2).contiguous().view(B_all, seq_len, C)
            return self.proj(out)
        
        attn.forward = types.MethodType(full_kv_forward, attn)
        attn._full_patched = True
        
        # Block forward
        def full_block_forward(self, x, kv_cache=None):
            if kv_cache is None:
                # å›é€€åˆ°åŸå§‹forward
                shortcut = x
                x = shortcut + self.drop_path(self.attn(x))
                shortcut = x
                x = shortcut + self.drop_path(self.ffn(x))
                return x
            else:
                shortcut = x
                attn_out = self.attn(x, kv_cache)
                x = shortcut + self.drop_path(attn_out)
                
                shortcut = x
                ffn_out = self.ffn(x)
                x = shortcut + self.drop_path(ffn_out)
                
                return x
        
        block.forward = types.MethodType(full_block_forward, block)

def full_forward_step_single(self, x, step):
    """å®Œæ•´çš„å•æ­¥å‰å‘ä¼ æ’­ï¼ˆ16å±‚ï¼‰"""
    for layer_idx, block in enumerate(self.blocks):
        x = block(x, kv_cache=self._kv_cache)
    
    logits = self.head(x)
    return logits

@torch.no_grad()
def autoregressive_infer_batch(self, tokens, max_steps=32):
    """çœŸæ­£çš„å•æ¬¡batchè°ƒç”¨ï¼åˆ é™¤æ‰€æœ‰for-loopï¼"""
    B_all = tokens.size(0)
    print(f"ğŸš€ TRUE BATCH INFERENCE: B_all={B_all}, max_steps={max_steps}")
    print("ğŸ”ª NO MORE FOR-LOOPS!")
    
    # åˆå§‹åŒ–
    x = self.encode_prompt(tokens[:, :1])  # [B_all, 1, C]
    generated = [tokens[:, :1]]
    
    # æ—¶é—´æ­¥å¾ªç¯ - è¿™æ˜¯å”¯ä¸€çš„å¾ªç¯ï¼
    for t in range(1, max_steps):
        if t % 10 == 0:
            print(f"â° Time step {t}/{max_steps}")
            # æ£€æŸ¥KV cacheçŠ¶æ€
            if "layer_0" in self._kv_cache and "k" in self._kv_cache["layer_0"]:
                cache_shape = self._kv_cache["layer_0"]["k"].shape
                print(f"   âœ… Layer 0 KV cache: {cache_shape}")
        
        # å•æ­¥å‰å‘ - å¤„ç†æ•´ä¸ªbatch
        logits = self.full_forward_step_single(x, t)  # [B_all, 1, vocab]
        logits = logits.squeeze(1)  # [B_all, vocab]
        
        # é‡‡æ ·
        probs = F.softmax(logits, dim=-1)
        next_tokens = torch.multinomial(probs, 1)  # [B_all, 1]
        
        generated.append(next_tokens)
        x = self.class_emb(next_tokens)
    
    result = torch.cat(generated, dim=1)  # [B_all, max_steps]
    print(f"âœ… Batch inference complete: {result.shape}")
    return result

# ä¸ºå®Œæ•´æµ‹è¯•å‡†å¤‡
_patch_all_layers_for_batch(var)
var.full_forward_step_single = types.MethodType(full_forward_step_single, var)
var.autoregressive_infer_batch = types.MethodType(autoregressive_infer_batch, var)

# ---------- ç¬¬ä¸‰æ­¥ï¼šæœ€ç»ˆæµ‹è¯•å¹¶è·å¾—3è¡Œç»“æœ ----------
def test_final_true_batch(P):
    print(f"\n{'='*60}")
    print(f"ğŸ FINAL TRUE BATCH TEST: P={P}")
    print(f"{'='*60}")
    
    # åˆ›å»ºPä¸ªè¾“å…¥
    tokens = torch.randint(100, 200, (P, 1), device=dev)
    print(f"ğŸ“ Input tokens: {tokens.flatten().tolist()}")
    
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.time()
    
    try:
        # å…³é”®ï¼šå•æ¬¡è°ƒç”¨ï¼
        outputs = var.autoregressive_infer_batch(tokens, max_steps=32)
        success = True
    except Exception as e:
        print(f"âŒ Error: {e}")
        success = False
        outputs = None
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    latency = (end_time - start_time) * 1000
    peak_mem = torch.cuda.max_memory_allocated() / 1e9
    
    # è®¡ç®—diversity
    diversity = 0.0
    if success and outputs is not None and P > 1:
        similarities = []
        for i in range(P):
            for j in range(i + 1, P):
                sim = F.cosine_similarity(
                    outputs[i].float(), outputs[j].float(), dim=0
                ).item()
                similarities.append(abs(sim))
        
        if similarities:
            avg_sim = sum(similarities) / len(similarities)
            diversity = 1.0 - avg_sim
    
    print(f"P={P:<2}  Lat {latency:>6.1f}ms  PeakMem {peak_mem:.2f}GB  Diversity {diversity:.3f}")
    
    return {
        'latency': latency,
        'peak_mem': peak_mem,
        'diversity': diversity,
        'success': success
    }

print("\nğŸ æœ€åä¸€åˆ€æµ‹è¯•ï¼šçœŸæ­£çš„å•æ¬¡batchè°ƒç”¨")
print("="*50)

results = {}
baseline_mem = None

for P in [1, 2, 4]:
    result = results[P] = test_final_true_batch(P)
    
    if P == 1:
        baseline_mem = result['peak_mem']
    else:
        if baseline_mem:
            mem_ratio = result['peak_mem'] / baseline_mem
            print(f"   Memory scaling: {mem_ratio:.2f}x baseline")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
            if P == 2 and mem_ratio >= 1.6:
                print(f"   âœ… P=2 memory scalingè¾¾æ ‡!")
            elif P == 4 and mem_ratio >= 2.5:
                print(f"   âœ… P=4 memory scalingè¾¾æ ‡!")
            elif mem_ratio < 1.2:
                print(f"   ğŸš¨ Memory scalingä¸è¶³ - KV cacheå¯èƒ½æœªç´¯ç§¯")
            else:
                print(f"   âš ï¸ Memory scalingéƒ¨åˆ†è¾¾æ ‡")

print(f"\nğŸ¯ æœ€ç»ˆ3è¡Œç»“æœ:")
print("-" * 50)
for P in [1, 2, 4]:
    r = results[P]
    print(f"P={P}  Lat {r['latency']:>6.1f}ms  PeakMem {r['peak_mem']:.2f}GB  Diversity {r['diversity']:.3f}")

# æ£€æŸ¥æ˜¯å¦å¯ä»¥å¼€é¦™æ§Ÿ
mem_scaling_ok = (
    results[2]['peak_mem'] / results[1]['peak_mem'] >= 1.6 and
    results[4]['peak_mem'] / results[1]['peak_mem'] >= 2.5
)

if mem_scaling_ok and all(r['success'] for r in results.values()):
    print(f"\nğŸ¾ CHAMPAGNE TIME! ğŸ¾")
    print(f"âœ… Memory scalingè¾¾æ ‡!")
    print(f"âœ… All tests successful!")
    print(f"ğŸ‰ VAR-ParScaleçœŸæ­£çš„å…±äº«éª¨å¹²å®Œæˆ! ğŸ‰")
else:
    print(f"\nğŸ”§ Still working...")
    if not mem_scaling_ok:
        print(f"âŒ Memory scaling needs work")
    for P, r in results.items():
        if not r['success']:
            print(f"âŒ P={P} failed")
SCRIPT_EOF

echo "ğŸ”ª æ‰§è¡Œæœ€åä¸€åˆ€..."
python3 final_last_cut.py
EOF

echo "âœ… æœ€åä¸€åˆ€æ‰§è¡Œå®Œæˆ!"