#!/bin/bash
# æ‰§è¡ŒçœŸæ­£çš„å…±äº«éª¨å¹²VAR-ParScaleï¼ˆåŸºäºæ­£ç¡®VARç»“æ„ï¼‰
set -e

SSH_CMD="ssh -p 11292 root@inst-gw.cloudexe.tech"
echo "ğŸš€ æ‰§è¡ŒçœŸæ­£çš„å…±äº«éª¨å¹²VAR-ParScaleï¼ˆåŸºäºæ­£ç¡®VARç»“æ„ï¼‰"
echo "="*60

$SSH_CMD << 'EOF'
cd /root/VAR

# åŸºäºæ­£ç¡®çš„VARç»“æ„åˆ›å»ºçœŸå…±äº«éª¨å¹²å®ç°
cat > test_true_shared_final.py << 'SCRIPT_EOF'
import torch, time, os, sys, torch.nn.functional as F
os.chdir("/root/VAR"); sys.path.append("/root/VAR")
from models import build_vae_var

# ---------- build models ----------
dev = "cuda"
vae, var = build_vae_var(V=4096,Cvae=32,ch=160,share_quant_resi=4,
                         device=dev,patch_nums=(1,2,3,4,5,6,8,10,13,16),
                         num_classes=1000,depth=16,shared_aln=False)
vae.cuda().eval(); var.cuda().eval()

# ---------- patch attention for batch processing ----------
import types

def _patch_attention_for_batch(model):
    """ä¿®è¡¥attentionä»¥æ”¯æŒbatch processingå’ŒKV caching"""
    for blk in model.blocks:
        attn = blk.attn
        if getattr(attn, "_patched", False):
            continue
            
        nh = attn.num_heads
        hdim = attn.head_dim
        orig_forward = attn.forward
        
        def batched_attn_forward(self, x, cache=None):
            # x: [B, 1, C]
            B, L, C = x.shape
            qkv = self.mat_qkv(x).view(B, L, 3, nh, hdim).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]  # each: [B, nh, 1, hdim]
            
            if cache is not None and "k" in cache:
                # æ‹¼æ¥å†å²KV cache
                k = torch.cat([cache["k"], k], dim=2)  # [B, nh, t+1, hdim]
                v = torch.cat([cache["v"], v], dim=2)  # [B, nh, t+1, hdim]
            
            if cache is not None:
                cache["k"], cache["v"] = k, v
            
            # scaled dot-product attention
            scale = hdim ** -0.5
            attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale
            attn_probs = F.softmax(attn_scores, dim=-1)
            out = torch.matmul(attn_probs, v)  # [B, nh, 1, hdim]
            
            # reshape and project
            out = out.transpose(1, 2).reshape(B, L, C)
            return self.proj(out)
        
        attn.forward = types.MethodType(batched_attn_forward, attn)
        attn._patched = True
        
        # ä¿®è¡¥blockçš„forwardä»¥æ”¯æŒcacheå‚æ•°
        orig_blk_forward = blk.forward
        def block_forward_with_cache(self, x, cache=None):
            if cache is None:
                return orig_blk_forward(x)
            else:
                # AdaLNSelfAttn forward with cache
                shortcut = x
                x = self.attn(x, cache)
                x = shortcut + self.drop_path(x)
                shortcut = x
                x = self.ffn(x)
                x = shortcut + self.drop_path(x)
                return x
        
        blk.forward = types.MethodType(block_forward_with_cache, blk)

_patch_attention_for_batch(var)

# ---------- add batched inference methods ----------
def encode_tokens(self, tokens):
    """åˆå§‹åŒ–KV cacheå¹¶ç¼–ç tokens"""
    self._kv_cache = [{} for _ in self.blocks]
    return self.class_emb(tokens)

def forward_step(self, x):
    """å‰å‘ä¼ æ’­ä¸€æ­¥ï¼Œä½¿ç”¨KV cache"""
    for layer_idx, block in enumerate(self.blocks):
        x = block(x, cache=self._kv_cache[layer_idx])
    return self.ln_f(x) @ self.class_emb.weight.T

@torch.no_grad()
def batched_autoregressive_infer(self, init_tokens, max_steps=128, top_p=0.95, top_k=900):
    """çœŸæ­£çš„æ‰¹é‡è‡ªå›å½’æ¨ç†"""
    B = init_tokens.size(0)
    x = encode_tokens(self, init_tokens[:, :1])  # [B, 1, C]
    generated = [init_tokens[:, :1]]  # list of [B, 1]
    
    for step in range(1, max_steps):
        logits = forward_step(self, x).squeeze(1)  # [B, vocab_size]
        
        # ç®€åŒ–é‡‡æ ·ï¼ˆå¯ä»¥æ”¹è¿›ä¸ºtop-k/top-pï¼‰
        probs = F.softmax(logits, dim=-1)
        next_tokens = torch.multinomial(probs, 1)  # [B, 1]
        
        generated.append(next_tokens)
        x = self.class_emb(next_tokens)  # å‡†å¤‡ä¸‹ä¸€æ­¥çš„è¾“å…¥
    
    return torch.cat(generated, dim=1)  # [B, max_steps]

# å°†æ–¹æ³•ç»‘å®šåˆ°VARå®ä¾‹
var.encode_tokens = types.MethodType(encode_tokens, var)
var.forward_step = types.MethodType(forward_step, var)
var.batched_autoregressive_infer = types.MethodType(batched_autoregressive_infer, var)

# ---------- run tests for P = 1/2/4 ----------
def run_test(P):
    tokens = torch.randint(0, 1000, (P, 1), device=dev)
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.time()
    outputs = var.batched_autoregressive_infer(tokens, max_steps=128)
    torch.cuda.synchronize()
    
    latency = (time.time() - start_time) * 1000
    peak_mem = torch.cuda.max_memory_allocated() / 1e9
    
    # è®¡ç®—diversity
    diversity = 0.0
    if P > 1:
        similarities = []
        for i in range(P):
            for j in range(i + 1, P):
                sim = F.cosine_similarity(
                    outputs[i].float(), outputs[j].float(), dim=0
                ).item()
                similarities.append(abs(sim))
        diversity = 1.0 - (sum(similarities) / len(similarities))
    
    print(f"P={P:<2}  Lat {latency:>6.1f}ms  PeakMem {peak_mem:.2f}GB  Diversity {diversity:.3f}")

print("ğŸš€ Running true shared backbone tests...")
for P in (1, 2, 4):
    run_test(P)
SCRIPT_EOF

echo "ğŸš€ æ‰§è¡ŒçœŸå…±äº«éª¨å¹²æµ‹è¯•..."
python3 test_true_shared_final.py
EOF

echo "âœ… çœŸå…±äº«éª¨å¹²æµ‹è¯•æ‰§è¡Œå®Œæˆ!"