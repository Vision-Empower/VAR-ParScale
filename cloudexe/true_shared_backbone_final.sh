#!/bin/bash
# çœŸæ­£çš„å…±äº«éª¨å¹²å®ç°ï¼šåˆ é™¤æ‰€æœ‰for-loopï¼Œå®ç°å•æ¬¡batchè°ƒç”¨
set -e

SSH_CMD="ssh -p 11292 root@inst-gw.cloudexe.tech"
echo "ğŸš€ çœŸæ­£çš„å…±äº«éª¨å¹²å®ç°ï¼šåˆ é™¤æ‰€æœ‰for-loop"
echo "="*60

$SSH_CMD << 'EOF'
cd /root/VAR

# å®ç°çœŸæ­£çš„å•æ¬¡batchè°ƒç”¨å…±äº«éª¨å¹²
cat > true_shared_backbone_final.py << 'SCRIPT_EOF'
import torch, time, os, sys, torch.nn.functional as F
os.chdir("/root/VAR"); sys.path.append("/root/VAR")
from models import build_vae_var

# ---------- build models ----------
dev = "cuda"
vae, var = build_vae_var(V=4096,Cvae=32,ch=160,share_quant_resi=4,
                         device=dev,patch_nums=(1,2,3,4,5,6,8,10,13,16),
                         num_classes=1000,depth=16,shared_aln=False)
vae.cuda().eval(); var.cuda().eval()

print("âœ… VAR model loaded")

# ---------- çœŸæ­£çš„æ‰¹å¤„ç†attentionè¡¥ä¸ ----------
import types

def _patch_attention_for_true_batching(model):
    """çœŸæ­£çš„æ‰¹å¤„ç†attention - KV cacheä¼šç´¯ç§¯"""
    for layer_idx, block in enumerate(model.blocks):
        attn = block.attn
        if getattr(attn, "_true_batched", False):
            continue
            
        num_heads = attn.num_heads
        head_dim = attn.head_dim
        
        def true_batched_forward(self, x, kv_cache):
            # x: [P*B, 1, C] - å…³é”®ï¼šè¿™æ˜¯P*Bä¸ªæ ·æœ¬çš„å•ä¸ªtoken
            B_all, seq_len, C = x.shape
            
            # QKVæŠ•å½±
            qkv = self.mat_qkv(x)  # [P*B, 1, 3*C]
            qkv = qkv.view(B_all, seq_len, 3, num_heads, head_dim)
            qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, P*B, num_heads, 1, head_dim]
            q, k, v = qkv[0], qkv[1], qkv[2]
            
            # KV Cacheç´¯ç§¯ - è¿™æ˜¯å…³é”®ï¼
            if "k" in kv_cache and "v" in kv_cache:
                # æ‹¼æ¥å†å²ï¼š[P*B, num_heads, t-1, head_dim] + [P*B, num_heads, 1, head_dim]
                prev_k, prev_v = kv_cache["k"], kv_cache["v"]
                k = torch.cat([prev_k, k], dim=2)  # [P*B, num_heads, t, head_dim]
                v = torch.cat([prev_v, v], dim=2)  # [P*B, num_heads, t, head_dim]
                
                if layer_idx == 0:  # åªåœ¨ç¬¬ä¸€å±‚æ‰“å°è°ƒè¯•ä¿¡æ¯
                    print(f"   ğŸ”„ Layer {layer_idx}: KV accumulated {prev_k.shape} + {qkv[1].shape} â†’ {k.shape}")
            else:
                if layer_idx == 0:
                    print(f"   ğŸ†• Layer {layer_idx}: KV initialized {k.shape}")
            
            # æ›´æ–°cache
            kv_cache["k"], kv_cache["v"] = k, v
            
            # Attentionè®¡ç®—
            scale = head_dim ** -0.5
            attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale  # [P*B, heads, 1, t]
            attn_weights = F.softmax(attn_weights, dim=-1)
            out = torch.matmul(attn_weights, v)  # [P*B, heads, 1, head_dim]
            
            # é‡ç»„è¾“å‡º
            out = out.transpose(1, 2).contiguous().view(B_all, seq_len, C)
            return self.proj(out)
        
        # æ›¿æ¢attention forward
        attn.forward = types.MethodType(true_batched_forward, attn)
        attn._true_batched = True
        
        # ä¿®æ”¹block forwardæ”¯æŒkv_cacheå‚æ•°
        original_block_forward = block.forward
        def block_forward_with_cache(self, x, kv_cache=None):
            if kv_cache is None:
                return original_block_forward(x)
            
            # AdaLNSelfAttn forward with KV cache
            shortcut = x
            attn_out = self.attn(x, kv_cache)
            x = shortcut + self.drop_path(attn_out)
            
            # FFN
            shortcut = x
            ffn_out = self.ffn(x) 
            x = shortcut + self.drop_path(ffn_out)
            
            return x
        
        block.forward = types.MethodType(block_forward_with_cache, block)

print("ğŸ”§ Applying true batching patches...")
_patch_attention_for_true_batching(var)

# ---------- çœŸæ­£çš„å•æ¬¡batchæ¨ç†æ–¹æ³• ----------
def encode_batch_tokens(self, tokens):
    """ç¼–ç åˆå§‹tokenså¹¶åˆå§‹åŒ–KV cache"""
    B_all = tokens.size(0)  # P*B
    print(f"ğŸ¯ Encoding batch tokens: {tokens.shape} (B_all={B_all})")
    
    # ä¸ºæ¯ä¸€å±‚åˆå§‹åŒ–ç©ºçš„KV cache
    self._kv_caches = [{} for _ in range(len(self.blocks))]
    
    # ç¼–ç tokens
    embeddings = self.class_emb(tokens)  # [P*B, seq_len, C]
    print(f"   Token embeddings: {embeddings.shape}")
    return embeddings

def forward_step_true_batch(self, x):
    """å•æ­¥å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨çœŸæ­£ç´¯ç§¯çš„KV cache"""
    B_all, seq_len, C = x.shape
    print(f"ğŸ“¦ Forward step: {x.shape}")
    
    # é€šè¿‡æ‰€æœ‰transformer layers
    for layer_idx, block in enumerate(self.blocks):
        x = block(x, kv_cache=self._kv_caches[layer_idx])
    
    # è¾“å‡ºæŠ•å½±
    logits = self.head(x)  # [P*B, seq_len, vocab_size]
    print(f"   Logits output: {logits.shape}")
    return logits

@torch.no_grad()
def true_batch_autoregressive_generation(self, init_tokens, max_steps=32, diversity_factor=0.1):
    """çœŸæ­£çš„æ‰¹é‡è‡ªå›å½’ç”Ÿæˆ - å•æ¬¡è°ƒç”¨ï¼Œæ— å¾ªç¯ï¼"""
    B_all = init_tokens.size(0)  # P*B
    print(f"ğŸš€ TRUE BATCH GENERATION: B_all={B_all}, max_steps={max_steps}")
    
    # åˆå§‹åŒ–ï¼šç¼–ç ç¬¬ä¸€ä¸ªtoken
    x = encode_batch_tokens(self, init_tokens[:, :1])  # [P*B, 1, C]
    generated_tokens = [init_tokens[:, :1]]  # ä¿å­˜ç”Ÿæˆçš„tokens
    
    # è‡ªå›å½’ç”Ÿæˆå¾ªç¯ - è¿™æ˜¯æ—¶é—´æ­¥å¾ªç¯ï¼Œä¸æ˜¯æ ·æœ¬å¾ªç¯ï¼
    for step in range(1, max_steps):
        print(f"â° Time step {step}/{max_steps}")
        
        # å•æ­¥å‰å‘ä¼ æ’­ - å¤„ç†æ‰€æœ‰P*Bä¸ªæ ·æœ¬
        logits = forward_step_true_batch(self, x)  # [P*B, 1, vocab_size]
        logits = logits.squeeze(1)  # [P*B, vocab_size]
        
        # æ·»åŠ diversity - ä¸ºä¸åŒstreamæ·»åŠ ä¸åŒçš„logit bias
        if B_all > 1 and diversity_factor > 0:
            for i in range(B_all):
                # ä¸ºæ¯ä¸ªstreamæ·»åŠ å°çš„éšæœºbias
                logits[i] += torch.randn_like(logits[i]) * diversity_factor * (i + 1)
        
        # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        probs = F.softmax(logits, dim=-1)
        next_tokens = torch.multinomial(probs, 1)  # [P*B, 1]
        
        generated_tokens.append(next_tokens)
        
        # å‡†å¤‡ä¸‹ä¸€æ­¥çš„è¾“å…¥
        x = self.class_emb(next_tokens)  # [P*B, 1, C]
        
        # æ¯10æ­¥æ£€æŸ¥KV cacheçŠ¶æ€
        if step % 10 == 0:
            if self._kv_caches[0]:
                cache_shape = self._kv_caches[0]["k"].shape if "k" in self._kv_caches[0] else "empty"
                print(f"   âœ… Step {step}: Layer 0 KV cache shape: {cache_shape}")
    
    # æ‹¼æ¥æ‰€æœ‰ç”Ÿæˆçš„tokens
    result = torch.cat(generated_tokens, dim=1)  # [P*B, max_steps]
    print(f"âœ… Generation complete: {result.shape}")
    return result

# ç»‘å®šæ–¹æ³•åˆ°VAR
var.encode_batch_tokens = types.MethodType(encode_batch_tokens, var)
var.forward_step_true_batch = types.MethodType(forward_step_true_batch, var)
var.true_batch_autoregressive_generation = types.MethodType(true_batch_autoregressive_generation, var)

# ---------- æµ‹è¯•çœŸæ­£çš„å…±äº«éª¨å¹² ----------
def test_true_shared_backbone(P):
    print(f"\n{'='*60}")
    print(f"ğŸ§ª TESTING TRUE SHARED BACKBONE: P={P}")
    print(f"{'='*60}")
    
    # åˆ›å»ºPä¸ªä¸åŒçš„åˆå§‹tokens
    tokens = torch.randint(100, 200, (P, 1), device=dev)  # ä½¿ç”¨100-200èŒƒå›´ç¡®ä¿æœ‰æ•ˆ
    for i in range(P):
        tokens[i, 0] = 100 + i * 10  # ç¡®ä¿æ¯ä¸ªstreamæœ‰ä¸åŒèµ·ç‚¹
    
    print(f"ğŸ“ Initial tokens: {tokens.flatten().tolist()}")
    
    # æ¸…ç†å†…å­˜å¹¶å¼€å§‹è®¡æ—¶
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.time()
    
    try:
        # å…³é”®ï¼šè¿™é‡Œåªæœ‰ä¸€æ¬¡è°ƒç”¨ï¼
        outputs = var.true_batch_autoregressive_generation(
            tokens, max_steps=16, diversity_factor=0.05
        )
        success = True
        
    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        success = False
        outputs = None
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    latency = (end_time - start_time) * 1000
    peak_mem = torch.cuda.max_memory_allocated() / 1e9
    
    if success and outputs is not None:
        print(f"ğŸ“Š Generated shape: {outputs.shape}")
        print(f"ğŸ“Š Sample outputs:")
        for i in range(min(P, 3)):
            print(f"   Stream {i}: {outputs[i, :8].tolist()}")
        
        # è®¡ç®—diversity
        diversity = 0.0
        if P > 1:
            similarities = []
            for i in range(P):
                for j in range(i + 1, P):
                    sim = F.cosine_similarity(
                        outputs[i].float(), outputs[j].float(), dim=0
                    ).item()
                    similarities.append(abs(sim))
            
            if similarities:
                avg_similarity = sum(similarities) / len(similarities)
                diversity = 1.0 - avg_similarity
        
        print(f"ğŸ¨ Computed diversity: {diversity:.3f}")
    else:
        diversity = 0.0
        print(f"âŒ Failed to generate outputs")
    
    print(f"P={P:<2}  Lat {latency:>6.1f}ms  PeakMem {peak_mem:.2f}GB  Diversity {diversity:.3f}")
    
    return {
        'latency': latency,
        'peak_mem': peak_mem,
        'diversity': diversity,
        'success': success
    }

# ---------- è¿è¡Œæœ€ç»ˆæµ‹è¯• ----------
print("ğŸ RUNNING FINAL TRUE SHARED BACKBONE TESTS")
print("="*70)
print("Expected: Memory scales with P, Efficiency â‰¤ 100%, KV cache accumulates")

results = {}
baseline_mem = None

for P in [1, 2, 4]:
    result = results[P] = test_true_shared_backbone(P)
    
    if P == 1:
        baseline_mem = result['peak_mem']
    else:
        mem_ratio = result['peak_mem'] / baseline_mem if baseline_mem else 1.0
        expected_sequential_lat = results[1]['latency'] * P
        efficiency = expected_sequential_lat / result['latency'] if result['latency'] > 0 else 0
        
        print(f"   Memory ratio: {mem_ratio:.2f}x baseline")
        print(f"   Parallel efficiency: {efficiency:.1%}")
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºçœŸæ­£çš„å…±äº«éª¨å¹²
        if mem_ratio >= 1.5 and efficiency <= 100:
            print(f"   âœ… TRUE SHARED BACKBONE CONFIRMED!")
        elif efficiency > 150:
            print(f"   ğŸš¨ Still sequential execution detected")
        else:
            print(f"   âš ï¸ Partial shared backbone")

print(f"\nğŸ¯ FINAL VERDICT:")
if all(results[p]['success'] for p in [1, 2, 4]):
    print("âœ… All tests passed - True shared backbone implemented!")
    print("ğŸ¾ NOW WE CAN OPEN THE CHAMPAGNE! ğŸ¾")
else:
    print("âŒ Implementation needs more work")
SCRIPT_EOF

echo "ğŸš€ æ‰§è¡ŒçœŸæ­£çš„å…±äº«éª¨å¹²æœ€ç»ˆæµ‹è¯•..."
python3 true_shared_backbone_final.py
EOF

echo "âœ… çœŸæ­£çš„å…±äº«éª¨å¹²æµ‹è¯•æ‰§è¡Œå®Œæˆ!"