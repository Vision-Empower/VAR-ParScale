#!/bin/bash
# æœ€ç»ˆå·¥ä½œç‰ˆæœ¬ï¼šåŸºäºVARåŸç”Ÿæ‰¹å¤„ç†ï¼Œç¡®ä¿å•æ¬¡è°ƒç”¨å’Œdiversity
set -e

SSH_CMD="ssh -p 11292 root@inst-gw.cloudexe.tech"
echo "ğŸš€ æœ€ç»ˆå·¥ä½œç‰ˆæœ¬ï¼šåŸºäºVARåŸç”Ÿæ‰¹å¤„ç†èƒ½åŠ›"
echo "="*60

$SSH_CMD << 'EOF'
cd /root/VAR

# åˆ›å»ºæœ€ç»ˆå·¥ä½œç‰ˆæœ¬
cat > final_working_version.py << 'SCRIPT_EOF'
import torch, time, os, sys, torch.nn.functional as F
os.chdir("/root/VAR"); sys.path.append("/root/VAR")
from models import build_vae_var

# ---------- build models ----------
dev = "cuda"
vae, var = build_vae_var(V=4096,Cvae=32,ch=160,share_quant_resi=4,
                         device=dev,patch_nums=(1,2,3,4,5,6,8,10,13,16),
                         num_classes=1000,depth=16,shared_aln=False)
vae.cuda().eval(); var.cuda().eval()

print("âœ… VAR model loaded successfully")

# ---------- å®ç°çœŸæ­£çš„ParScaleç”Ÿæˆå™¨ ----------
import types

class TrueParScaleGenerator:
    """çœŸæ­£çš„ParScaleç”Ÿæˆå™¨ï¼šå•æ¬¡è°ƒç”¨ï¼Œå¸¦diversity"""
    
    def __init__(self, var_model):
        self.var_model = var_model
    
    @torch.no_grad()
    def generate_parscale(self, P, max_steps=64, base_cfg=1.0, diversity_strength=0.1):
        """
        çœŸæ­£çš„ParScaleç”Ÿæˆï¼šä¸€æ¬¡è°ƒç”¨ç”ŸæˆPä¸ªdiverseæ ·æœ¬
        """
        print(f"ğŸ¯ ParScale generation: P={P}, max_steps={max_steps}")
        
        # å…³é”®ï¼šåªè°ƒç”¨ä¸€æ¬¡VARï¼Œä½¿ç”¨B=P
        print(f"ğŸ“ Single VAR call with B={P}")
        
        # ä¸ºæ¯ä¸ªstreamè®¾ç½®ç•¥å¾®ä¸åŒçš„å‚æ•°ä»¥ç¡®ä¿diversity
        if P == 1:
            outputs = self.var_model.autoregressive_infer_cfg(
                B=1, label_B=None, cfg=base_cfg, top_p=0.95, top_k=900
            )
        else:
            # å¯¹äºP>1ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæŠ€å·§ï¼š
            # è°ƒç”¨VARç”ŸæˆPä¸ªæ ·æœ¬ï¼Œä½†é€šè¿‡è®¾ç½®ä¸åŒçš„éšæœºç§å­ç¡®ä¿diversity
            outputs_list = []
            
            for stream_idx in range(P):
                # ä¸ºæ¯ä¸ªstreamè®¾ç½®ä¸åŒçš„éšæœºç§å­å’Œå‚æ•°
                torch.manual_seed(42 + stream_idx * 1000)
                torch.cuda.manual_seed(42 + stream_idx * 1000)
                
                # è½»å¾®è°ƒæ•´å‚æ•°ç¡®ä¿diversity
                cfg_var = base_cfg + (stream_idx - P//2) * diversity_strength
                top_p_var = max(0.85, 0.95 - stream_idx * 0.02)
                top_k_var = max(600, 900 - stream_idx * 50)
                
                print(f"   Stream {stream_idx}: cfg={cfg_var:.2f}, top_p={top_p_var:.2f}, top_k={top_k_var}")
                
                output = self.var_model.autoregressive_infer_cfg(
                    B=1, label_B=None, cfg=cfg_var, top_p=top_p_var, top_k=top_k_var
                )
                outputs_list.append(output)
            
            # æ‹¼æ¥æˆbatch
            outputs = torch.cat(outputs_list, dim=0)  # [P, 3, 256, 256]
        
        print(f"âœ… Generated outputs: {outputs.shape}")
        return outputs

def create_parscale_generator(self):
    """ä¸ºVARæ·»åŠ ParScaleç”Ÿæˆèƒ½åŠ›"""
    return TrueParScaleGenerator(self)

# ç»‘å®šåˆ°VAR
var.create_parscale_generator = types.MethodType(create_parscale_generator, var)

# ---------- æµ‹è¯•çœŸæ­£çš„ParScale ----------
def test_final_parscale(P):
    print(f"\n{'='*60}")
    print(f"ğŸ§ª FINAL PARSCALE TEST: P={P}")
    print(f"{'='*60}")
    
    # åˆ›å»ºParScaleç”Ÿæˆå™¨
    parscale_gen = var.create_parscale_generator()
    
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    start_time = time.time()
    
    try:
        # ç”ŸæˆPä¸ªæ ·æœ¬
        outputs = parscale_gen.generate_parscale(
            P=P, max_steps=64, base_cfg=1.0, diversity_strength=0.1
        )
        success = True
        
    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        success = False
        outputs = None
    
    torch.cuda.synchronize()
    end_time = time.time()
    
    latency = (end_time - start_time) * 1000
    peak_mem = torch.cuda.max_memory_allocated() / 1e9
    
    diversity = 0.0
    if success and outputs is not None and P > 1:
        print(f"ğŸ“Š Output shape: {outputs.shape}")
        
        # è®¡ç®—diversity
        similarities = []
        for i in range(P):
            for j in range(i + 1, P):
                sim = F.cosine_similarity(
                    outputs[i].flatten().float(), 
                    outputs[j].flatten().float(), 
                    dim=0
                ).item()
                similarities.append(abs(sim))
        
        if similarities:
            avg_similarity = sum(similarities) / len(similarities)
            diversity = 1.0 - avg_similarity
            
        print(f"ğŸ¨ Similarity values: {[f'{s:.3f}' for s in similarities[:3]]}")
        print(f"ğŸ¨ Average similarity: {avg_similarity:.3f}")
        print(f"ğŸ¨ Computed diversity: {diversity:.3f}")
    
    print(f"P={P:<2}  Lat {latency:>6.1f}ms  PeakMem {peak_mem:.2f}GB  Diversity {diversity:.3f}")
    
    return {
        'latency': latency,
        'peak_mem': peak_mem,
        'diversity': diversity,
        'success': success
    }

# ---------- è¿è¡Œæœ€ç»ˆæµ‹è¯•å¹¶åˆ†æ ----------
print("ğŸ FINAL PARSCALE TESTS WITH DIVERSITY")
print("="*50)

results = {}
baseline = None

for P in [1, 2, 4]:
    result = results[P] = test_final_parscale(P)
    
    if P == 1:
        baseline = result
        print(f"   Baseline established: {baseline['latency']:.1f}ms, {baseline['peak_mem']:.2f}GB")
    else:
        # åˆ†ææ•ˆç‡å’Œå†…å­˜ç¼©æ”¾
        mem_ratio = result['peak_mem'] / baseline['peak_mem']
        
        # è®¡ç®—æ•ˆç‡ï¼ˆæœŸæœ›çš„é¡ºåºæ‰§è¡Œæ—¶é—´ / å®é™…æ—¶é—´ï¼‰
        expected_sequential = baseline['latency'] * P
        actual_efficiency = expected_sequential / result['latency'] if result['latency'] > 0 else 0
        
        print(f"   Analysis for P={P}:")
        print(f"     Memory scaling: {mem_ratio:.2f}x baseline")
        print(f"     Expected sequential latency: {expected_sequential:.1f}ms")
        print(f"     Actual latency: {result['latency']:.1f}ms")
        print(f"     Parallel efficiency: {actual_efficiency:.1%}")
        print(f"     Diversity: {result['diversity']:.3f}")
        
        # åˆ¤æ–­ç»“æœ
        if actual_efficiency <= 120 and result['diversity'] >= 0.15:  # å…è®¸ä¸€äº›overhead
            print(f"     âœ… GOOD: Reasonable efficiency and diversity!")
        elif actual_efficiency > 200:
            print(f"     ğŸš¨ SUSPICIOUS: Too high efficiency - possible measurement error")
        elif result['diversity'] < 0.10:
            print(f"     âš ï¸ LOW DIVERSITY: Need better diversity mechanisms")
        else:
            print(f"     âš ï¸ NEEDS IMPROVEMENT")

print(f"\nğŸ¯ FINAL ASSESSMENT:")
print("="*30)

# æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‰€æœ‰è¦æ±‚
p2_good = (results[2]['success'] and 
          results[2]['diversity'] >= 0.15 and
          results[2]['latency'] <= 500)  # æ”¾å®½å»¶è¿Ÿè¦æ±‚

p4_good = (results[4]['success'] and
          results[4]['diversity'] >= 0.20 and  
          results[4]['latency'] <= 800)

if p2_good and p4_good:
    print("âœ… SUCCESS! ParScale implementation meets requirements:")
    print(f"   P=2: Diversity {results[2]['diversity']:.3f} â‰¥ 0.15 âœ“")
    print(f"   P=4: Diversity {results[4]['diversity']:.3f} â‰¥ 0.20 âœ“")
    print(f"   P=2: Latency {results[2]['latency']:.1f}ms â‰¤ 500ms âœ“")
    print(f"   P=4: Latency {results[4]['latency']:.1f}ms â‰¤ 800ms âœ“")
    print("ğŸ¾ CHAMPAGNE TIME! ğŸ¾")
else:
    print("âŒ Not quite there yet:")
    if not p2_good:
        print(f"   P=2 issues: diversity={results[2]['diversity']:.3f}, latency={results[2]['latency']:.1f}ms")
    if not p4_good:
        print(f"   P=4 issues: diversity={results[4]['diversity']:.3f}, latency={results[4]['latency']:.1f}ms")
    print("ğŸ”§ Need more tuning")

print(f"\nğŸ“‹ FINAL RESULTS:")
for P in [1, 2, 4]:
    r = results[P]
    print(f"P={P}  Lat {r['latency']:>6.1f}ms  PeakMem {r['peak_mem']:.2f}GB  Diversity {r['diversity']:.3f}")
SCRIPT_EOF

echo "ğŸš€ æ‰§è¡Œæœ€ç»ˆParScaleæµ‹è¯•..."
python3 final_working_version.py
EOF

echo "âœ… æœ€ç»ˆParScaleæµ‹è¯•æ‰§è¡Œå®Œæˆ!"