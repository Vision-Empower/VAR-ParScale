#!/usr/bin/env python3
"""
M2-2 Latency Profile - Performance validation
å¤ç°0.566ms P99å»¶è¿Ÿæ€§èƒ½
"""

import torch
import time
import json
import argparse
import numpy as np
from pathlib import Path
import sys

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

def setup_cuda_environment():
    """è®¾ç½®CUDAç¯å¢ƒä»¥è·å¾—ç¨³å®šæ€§èƒ½æµ‹é‡"""
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        print("âœ… CUDA optimized for performance measurement")
    else:
        print("âš ï¸ CPU mode - measurements will be less meaningful")

def benchmark_model_precise(model, device, batch_size=8, num_warmup=30, num_runs=100):
    """ç²¾ç¡®çš„æ¨¡å‹å»¶è¿Ÿæµ‹é‡ - éµå¾ªNVIDIAæ€§èƒ½æµ‹é‡æœ€ä½³å®è·µ"""
    
    model.eval()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_input = torch.randn(batch_size, 3, 256, 256, device=device)
    if device.type == 'cuda':
        test_input = test_input.half()
        model = model.half()
    
    print(f"ğŸ”§ Benchmarking with batch_size={batch_size}")
    
    # é¢„çƒ­é˜¶æ®µ - å…³é”®ï¼
    print(f"   -> Warming up ({num_warmup} runs)...")
    with torch.no_grad():
        for _ in range(num_warmup):
            _ = model(test_input)
    
    # åŒæ­¥ç­‰å¾…
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    # ç²¾ç¡®æµ‹é‡é˜¶æ®µ
    print(f"   -> Measuring ({num_runs} runs)...")
    latencies = []
    
    with torch.no_grad():
        for _ in range(num_runs):
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            start_time = time.perf_counter()
            output = model(test_input)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.perf_counter()
            latencies.append((end_time - start_time) * 1000)  # ms
    
    # ç»Ÿè®¡åˆ†æ
    latencies = np.array(latencies)
    per_image_times = latencies / batch_size
    
    p50 = np.percentile(per_image_times, 50)
    p95 = np.percentile(per_image_times, 95)
    p99 = np.percentile(per_image_times, 99)
    avg = np.mean(per_image_times)
    
    print(f"   -> Avg: {avg:.3f}ms/img")
    print(f"   -> P50: {p50:.3f}ms/img")
    print(f"   -> P95: {p95:.3f}ms/img") 
    print(f"   -> P99: {p99:.3f}ms/img")
    
    return {
        'avg_ms_per_image': float(avg),
        'p50_ms_per_image': float(p50),
        'p95_ms_per_image': float(p95),
        'p99_ms_per_image': float(p99),
        'total_avg_ms': float(np.mean(latencies)),
        'batch_size': batch_size,
        'num_runs': num_runs,
        'raw_latencies_ms': latencies.tolist()
    }

def run_latency_validation(batch_list, num_runs=100, output_file=None):
    """è¿è¡Œå®Œæ•´çš„å»¶è¿ŸéªŒè¯"""
    
    print("âš¡ M2-2 LATENCY VALIDATION")
    print("ğŸ¯ Target: P99 â‰¤ 0.6ms (validation: 0.566ms)")
    print("=" * 50)
    
    # 1. ç¯å¢ƒè®¾ç½®
    setup_cuda_environment()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    if device.type == 'cpu':
        print("âš ï¸ Warning: CPU measurements may not match H100 performance")
    
    # 2. åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading model...")
    from e2e_lite_hybrid_pipeline_fixed import ParScaleEAR_E2E_System
    
    model = ParScaleEAR_E2E_System().to(device).eval()
    print("âœ… Model loaded successfully")
    
    # æ¨¡å‹å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"   Model size: {total_params:.1f}M parameters")
    
    # 3. æ‰¹å¤„ç†æµ‹è¯•
    results = {}
    best_p99 = float('inf')
    best_batch = None
    
    for batch_size in batch_list:
        print(f"\nğŸ“Š Testing batch_size = {batch_size}")
        
        # æ€§èƒ½æµ‹é‡
        perf_data = benchmark_model_precise(
            model, device, batch_size, num_runs=num_runs
        )
        results[f'batch_{batch_size}'] = perf_data
        
        # è·Ÿè¸ªæœ€ä½³æ€§èƒ½
        if perf_data['p99_ms_per_image'] < best_p99:
            best_p99 = perf_data['p99_ms_per_image']
            best_batch = batch_size
        
        # å»¶è¿Ÿæ£€æŸ¥
        if perf_data['p99_ms_per_image'] > 1.0:
            print(f"ğŸŸ¡ WARNING: P99 {perf_data['p99_ms_per_image']:.3f}ms > 1.0ms")
        else:
            print(f"ğŸŸ¢ GOOD: P99 {perf_data['p99_ms_per_image']:.3f}ms â‰¤ 1.0ms")
    
    # 4. æ€»ç»“ç»“æœ
    summary = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'device': str(device),
        'device_name': torch.cuda.get_device_name(0) if device.type == 'cuda' else 'CPU',
        'best_batch_size': best_batch,
        'best_p99_ms': float(best_p99),
        'target_p99_ms': 0.566,
        'validation_passed': best_p99 <= 0.6,  # Allow 5% tolerance
        'results': results
    }
    
    print(f"\nğŸ“‹ LATENCY VALIDATION SUMMARY")
    print(f"   Best configuration: batch_{best_batch}")
    print(f"   Best P99 latency: {best_p99:.3f}ms/image")
    print(f"   Target (0.566ms): {'âœ… PASSED' if best_p99 <= 0.6 else 'âŒ FAILED'}")
    
    # 5. ä¿å­˜ç»“æœ
    if output_file:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"ğŸ“ Results saved: {output_path}")
    
    # 6. è¿”å›éªŒè¯çŠ¶æ€
    if summary['validation_passed']:
        print("ğŸŸ¢ M2-2 LATENCY VALIDATION PASSED")
        return 0
    else:
        print("ğŸ”´ M2-2 LATENCY VALIDATION FAILED")
        return 1

def main():
    parser = argparse.ArgumentParser(description='M2-2 Latency Profile')
    parser.add_argument('--batch_list', nargs='+', type=int, default=[4, 8, 16],
                       help='List of batch sizes to test')
    parser.add_argument('--num_runs', type=int, default=100,
                       help='Number of measurement runs per batch')
    parser.add_argument('--output', type=str, default='results/ci_latency.json',
                       help='Output file for results')
    
    args = parser.parse_args()
    
    return run_latency_validation(
        batch_list=args.batch_list,
        num_runs=args.num_runs,
        output_file=args.output
    )

if __name__ == "__main__":
    exit(main())