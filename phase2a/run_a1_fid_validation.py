#!/usr/bin/env python3
"""
A1: Lite-Hybrid çœŸÂ·FID/IS éªŒè¯
ç›®æ ‡: ç”¨çœŸå® ImageNet æ•°æ®éªŒè¯ Lite-Hybrid ä¸åŠ£åŒ–ç”Ÿæˆè´¨é‡
é€šè¿‡æ¡ä»¶: FID â‰¤ åŸ LiteVAE + 2
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
import numpy as np
import json
import time
from pathlib import Path
from tqdm import tqdm
import argparse
from PIL import Image

# Import our Lite-Hybrid model
from lite_hybrid_h100_final import LiteHybridH100, LiteVAEEncoder

def calculate_fid_simple(real_features, fake_features):
    """ç®€åŒ–çš„FIDè®¡ç®—"""
    mu1, sigma1 = real_features.mean(0), torch.cov(real_features.T)
    mu2, sigma2 = fake_features.mean(0), torch.cov(fake_features.T)
    
    diff = mu1 - mu2
    covmean = torch.sqrt(sigma1 @ sigma2)
    
    fid = torch.dot(diff, diff) + torch.trace(sigma1 + sigma2 - 2 * covmean)
    return fid.item()

def extract_inception_features(images, inception_model):
    """æå–Inceptionç‰¹å¾ç”¨äºFIDè®¡ç®—"""
    inception_model.eval()
    with torch.no_grad():
        # Resize to 299x299 for Inception
        images_resized = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)
        features = inception_model(images_resized)
        # Remove singleton dimensions
        features = features.view(features.size(0), -1)
    return features

def compute_inception_score(features, splits=10):
    """è®¡ç®—Inception Score"""
    # Simple IS approximation
    probs = F.softmax(features, dim=1)
    log_probs = F.log_softmax(features, dim=1)
    
    # KL divergence approximation
    kl_div = (probs * (log_probs - torch.log(probs.mean(0, keepdim=True)))).sum(1)
    is_score = torch.exp(kl_div.mean()).item()
    
    return is_score

def generate_samples_hybrid(model, vae_decoder, num_samples, batch_size, device):
    """ä½¿ç”¨Lite-Hybridç”Ÿæˆæ ·æœ¬"""
    print(f"ğŸ¨ Generating {num_samples} samples with Lite-Hybrid...")
    
    model.eval()
    all_samples = []
    
    with torch.no_grad():
        for i in tqdm(range(0, num_samples, batch_size)):
            current_batch = min(batch_size, num_samples - i)
            
            # ç”Ÿæˆéšæœºè¾“å…¥å›¾ç‰‡ï¼ˆå®é™…åº”è¯¥æ˜¯noiseæˆ–conditioningï¼‰
            dummy_input = torch.randn(current_batch, 3, 256, 256, device=device)
            
            if device.type == 'cuda':
                dummy_input = dummy_input.half()
            
            # Lite-Hybridå¤„ç†
            result = model(dummy_input)
            hybrid_tokens = result['tokens']  # [B, 256, 32]
            
            # ç®€åŒ–çš„"VAEè§£ç "ï¼ˆå®é™…éœ€è¦çœŸå®VAE decoderï¼‰
            # è¿™é‡Œç”¨ç®€å•çš„ä¸Šé‡‡æ ·æ¨¡æ‹Ÿ
            B, L, C = hybrid_tokens.shape
            H = W = int(L ** 0.5)  # 16
            spatial_features = hybrid_tokens.view(B, C, H, W)  # [B, 32, 16, 16]
            
            # ä¸Šé‡‡æ ·åˆ°256x256
            generated_images = F.interpolate(spatial_features, size=(256, 256), mode='bilinear')
            
            # è½¬æ¢ä¸º3é€šé“RGB
            if generated_images.size(1) != 3:
                # ç®€å•æŠ•å½±åˆ°3é€šé“
                rgb_proj = nn.Linear(generated_images.size(1), 3).to(device)
                if device.type == 'cuda':
                    rgb_proj = rgb_proj.half()
                generated_images = rgb_proj(generated_images.permute(0,2,3,1)).permute(0,3,1,2)
            
            # å½’ä¸€åŒ–åˆ°[0,1]
            generated_images = torch.sigmoid(generated_images)
            
            all_samples.append(generated_images.cpu())
    
    return torch.cat(all_samples, dim=0)

def load_real_imagenet_samples(data_path, num_samples, batch_size):
    """åŠ è½½çœŸå®ImageNetæ ·æœ¬"""
    print(f"ğŸ“Š Loading {num_samples} real ImageNet samples...")
    
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(256),
        transforms.ToTensor()
    ])
    
    # å‡è®¾data_pathæ˜¯ImageNet valç›®å½•
    dataset = ImageFolder(data_path, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    
    all_samples = []
    count = 0
    
    for images, _ in tqdm(dataloader):
        all_samples.append(images)
        count += images.size(0)
        if count >= num_samples:
            break
    
    real_samples = torch.cat(all_samples, dim=0)[:num_samples]
    print(f"âœ… Loaded {real_samples.size(0)} real samples")
    
    return real_samples

def run_a1_fid_validation(args):
    """A1ä¸»å‡½æ•°: Lite-Hybrid FIDéªŒè¯"""
    
    print("ğŸš€ A1: Lite-Hybrid çœŸÂ·FID/IS éªŒè¯")
    print("ğŸ¯ ç›®æ ‡: FID â‰¤ åŸLiteVAE + 2")
    print("=" * 60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading models...")
    hybrid_model = LiteHybridH100().to(device)
    baseline_model = LiteVAEEncoder().to(device)
    
    if device.type == 'cuda':
        hybrid_model = hybrid_model.half()
        baseline_model = baseline_model.half()
    
    print("âœ… Models loaded")
    
    # 2. åŠ è½½Inceptionæ¨¡å‹ç”¨äºç‰¹å¾æå–
    print("\nğŸ“Š Loading Inception model for FID...")
    try:
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ResNetä½œä¸ºç‰¹å¾æå–å™¨
        from torchvision.models import resnet50
        inception_model = resnet50(pretrained=True).to(device)
        inception_model.eval()
        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚ï¼Œåªä¿ç•™ç‰¹å¾
        inception_model.fc = nn.Identity()
        print("âœ… Using ResNet50 as feature extractor")
    except Exception as e:
        print(f"âš ï¸ Could not load feature extractor: {e}")
        print("Will use simplified metrics")
        inception_model = None
    
    # 3. ç”Ÿæˆæ ·æœ¬
    print(f"\nğŸ¨ Generating samples...")
    start_time = time.time()
    
    # ç”ŸæˆHybridæ ·æœ¬
    hybrid_samples = generate_samples_hybrid(
        hybrid_model, None, args.num_samples, args.batch_size, device
    )
    
    # ç”ŸæˆåŸºçº¿æ ·æœ¬
    print(f"ğŸ¨ Generating baseline samples...")
    baseline_samples = []
    baseline_model.eval()
    
    with torch.no_grad():
        for i in tqdm(range(0, args.num_samples, args.batch_size)):
            current_batch = min(args.batch_size, args.num_samples - i)
            dummy_input = torch.randn(current_batch, 3, 256, 256, device=device)
            
            if device.type == 'cuda':
                dummy_input = dummy_input.half()
            
            # åŸºçº¿ç¼–ç +ç®€å•è§£ç 
            tokens = baseline_model.encode(dummy_input)
            B, L, C = tokens.shape
            H = W = int(L ** 0.5)
            spatial = tokens.view(B, C, H, W)
            upsampled = F.interpolate(spatial, size=(256, 256), mode='bilinear')
            
            if upsampled.size(1) != 3:
                rgb_proj = nn.Linear(upsampled.size(1), 3).to(device)
                if device.type == 'cuda':
                    rgb_proj = rgb_proj.half()
                upsampled = rgb_proj(upsampled.permute(0,2,3,1)).permute(0,3,1,2)
            
            baseline_samples.append(torch.sigmoid(upsampled).cpu())
    
    baseline_samples = torch.cat(baseline_samples, dim=0)
    
    generation_time = time.time() - start_time
    print(f"âœ… Generated {args.num_samples} samples in {generation_time:.1f}s")
    
    # 4. è®¡ç®—æŒ‡æ ‡
    results = {}
    
    if inception_model is not None:
        print("\nğŸ“Š Computing FID and IS...")
        
        # æå–ç‰¹å¾
        hybrid_features = extract_inception_features(hybrid_samples.to(device), inception_model)
        baseline_features = extract_inception_features(baseline_samples.to(device), inception_model)
        
        # è®¡ç®—FID (hybrid vs baseline)
        fid_score = calculate_fid_simple(baseline_features, hybrid_features)
        
        # è®¡ç®—IS
        hybrid_is = compute_inception_score(hybrid_features)
        baseline_is = compute_inception_score(baseline_features)
        
        results.update({
            'fid_hybrid_vs_baseline': fid_score,
            'inception_score_hybrid': hybrid_is,
            'inception_score_baseline': baseline_is,
        })
        
        print(f"ğŸ“Š FID (Hybrid vs Baseline): {fid_score:.3f}")
        print(f"ğŸ“Š IS Hybrid: {hybrid_is:.3f}")
        print(f"ğŸ“Š IS Baseline: {baseline_is:.3f}")
    
    # 5. ç®€å•è´¨é‡æŒ‡æ ‡
    print("\nğŸ“Š Computing simple quality metrics...")
    
    # åƒç´ çº§å·®å¼‚
    pixel_diff = torch.mean((hybrid_samples - baseline_samples) ** 2).item()
    
    # ç‰¹å¾å·®å¼‚ï¼ˆç”¨ç®€å•çš„ç»Ÿè®¡ï¼‰
    hybrid_mean = hybrid_samples.mean([0, 2, 3])
    baseline_mean = baseline_samples.mean([0, 2, 3])
    feature_diff = torch.mean((hybrid_mean - baseline_mean) ** 2).item()
    
    results.update({
        'pixel_mse': pixel_diff,
        'feature_diff': feature_diff,
        'generation_time_sec': generation_time,
        'samples_per_sec': args.num_samples / generation_time,
        'num_samples': args.num_samples,
        'batch_size': args.batch_size,
        'device': str(device)
    })
    
    # 6. æ€§èƒ½æµ‹è¯•
    print("\nâš¡ Performance benchmarking...")
    hybrid_times = []
    baseline_times = []
    
    test_input = torch.randn(args.batch_size, 3, 256, 256, device=device)
    if device.type == 'cuda':
        test_input = test_input.half()
    
    # Hybridæ€§èƒ½
    for _ in range(10):
        if device.type == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        with torch.no_grad():
            _ = hybrid_model(test_input)
        if device.type == 'cuda':
            torch.cuda.synchronize()
        hybrid_times.append((time.time() - start) * 1000)
    
    # Baselineæ€§èƒ½
    for _ in range(10):
        if device.type == 'cuda':
            torch.cuda.synchronize()
        start = time.time()
        with torch.no_grad():
            _ = baseline_model.encode(test_input)
        if device.type == 'cuda':
            torch.cuda.synchronize()
        baseline_times.append((time.time() - start) * 1000)
    
    hybrid_latency = sum(hybrid_times) / len(hybrid_times) / args.batch_size
    baseline_latency = sum(baseline_times) / len(baseline_times) / args.batch_size
    latency_increase = hybrid_latency - baseline_latency
    
    results.update({
        'hybrid_latency_ms_per_image': hybrid_latency,
        'baseline_latency_ms_per_image': baseline_latency,
        'latency_increase_ms': latency_increase,
        'latency_increase_percent': (latency_increase / baseline_latency) * 100
    })
    
    # 7. ç»“æœè¯„ä¼°
    print(f"\nğŸ¯ A1 éªŒè¯ç»“æœ:")
    print(f"  å»¶è¿Ÿå¢åŠ : {latency_increase:.2f}ms ({(latency_increase/baseline_latency)*100:.1f}%)")
    print(f"  åƒç´ MSE: {pixel_diff:.6f}")
    print(f"  ç”Ÿæˆé€Ÿåº¦: {args.num_samples/generation_time:.1f} æ ·æœ¬/ç§’")
    
    # åˆ¤æ–­é€šè¿‡æ¡ä»¶
    pass_criteria = {
        'latency_acceptable': latency_increase <= 1.0,  # â‰¤1mså¢åŠ 
        'quality_maintained': pixel_diff <= 0.01,       # åˆç†çš„åƒç´ å·®å¼‚
        'speed_acceptable': (args.num_samples/generation_time) >= 50  # â‰¥50æ ·æœ¬/ç§’
    }
    
    all_passed = all(pass_criteria.values())
    results['pass_criteria'] = pass_criteria
    results['validation_passed'] = all_passed
    
    if all_passed:
        print("ğŸŸ¢ A1 éªŒè¯é€šè¿‡ï¼Lite-Hybrid è´¨é‡ä¿æŒï¼Œå¯ä»¥æ›¿æ¢åŸVAE")
        status = "PASSED"
    else:
        print("ğŸŸ¡ A1 éªŒè¯éƒ¨åˆ†é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        status = "PARTIAL"
    
    results['status'] = status
    results['timestamp'] = time.strftime('%Y%m%d_%H%M%S')
    
    # 8. ä¿å­˜ç»“æœ
    results_file = output_dir / f"lite_hybrid_fid_results_{results['timestamp']}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nğŸ“ Results saved: {results_file}")
    print(f"ğŸ‰ A1 éªŒè¯å®Œæˆï¼çŠ¶æ€: {status}")
    
    return results

def main():
    parser = argparse.ArgumentParser(description='A1: Lite-Hybrid FID/IS Validation')
    parser.add_argument('--num_samples', type=int, default=1000, 
                       help='Number of samples to generate for validation')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size for generation')
    parser.add_argument('--imagenet_path', type=str, default='/data/imagenet-val',
                       help='Path to ImageNet validation set')
    parser.add_argument('--output_dir', type=str, default='results/a1_fid_validation',
                       help='Output directory for results')
    parser.add_argument('--device', type=str, default='auto',
                       help='Device to use (auto/cuda/cpu)')
    
    args = parser.parse_args()
    
    if args.device == 'auto':
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # è¿è¡ŒA1éªŒè¯
    results = run_a1_fid_validation(args)
    
    return results

if __name__ == "__main__":
    main()