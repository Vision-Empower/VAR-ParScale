#!/usr/bin/env python3
"""
Lite-Hybrid H100 Test - Final GPU version of #6 experiment
HART-inspired dual-branch architecture with 256x256 images
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import time
import json
import numpy as np

# Force CUDA for H100
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"ğŸš€ Using H100 GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
else:
    device = torch.device('cpu')
    print("âš ï¸ CUDA not available, falling back to CPU")

class LiteVAEEncoder(nn.Module):
    """Optimized VAE encoder for H100"""
    def __init__(self, in_channels=3, z_channels=32):
        super().__init__()
        # More realistic encoder for 256x256 â†’ 16x16
        self.encoder = nn.Sequential(
            # 256 â†’ 128
            nn.Conv2d(in_channels, 64, 4, stride=2, padding=1),
            nn.GroupNorm(8, 64),
            nn.SiLU(),
            
            # 128 â†’ 64  
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.GroupNorm(8, 128),
            nn.SiLU(),
            
            # 64 â†’ 32
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.GroupNorm(8, 256),
            nn.SiLU(),
            
            # 32 â†’ 16
            nn.Conv2d(256, z_channels, 4, stride=2, padding=1)
        )
        
    def encode(self, x):
        """Encode images to latent tokens"""
        h = self.encoder(x)  # [B, 32, 16, 16]
        B, C, H, W = h.shape
        return h.view(B, H*W, C)  # [B, 256, 32]

class CoarseTokenizer(nn.Module):
    """HART-inspired coarse branch - 16x16 â†’ 4x4"""
    
    def __init__(self, in_channels=32, coarse_vocab_size=1024):
        super().__init__()
        self.coarse_vocab_size = coarse_vocab_size
        
        # ä¸‹é‡‡æ ·åˆ°4x4 (16x16 â†’ 4x4 via 4x4 conv)
        self.downsample = nn.Sequential(
            nn.Conv2d(in_channels, 128, 4, stride=4),  # 16x16 â†’ 4x4
            nn.GroupNorm(8, 128),
            nn.SiLU(),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.GroupNorm(8, 256),
            nn.SiLU()
        )
        
        # ç¦»æ•£é‡åŒ–
        self.quantizer = nn.Linear(256, coarse_vocab_size)
        
    def forward(self, fine_latent):
        """ä»fine latentæå–coarse tokens"""
        # Reshape [B, 256, 32] â†’ [B, 32, 16, 16]
        B, L, C = fine_latent.shape
        H = W = int(L ** 0.5)  # 16
        fine_latent = fine_latent.view(B, C, H, W)
        
        coarse_feat = self.downsample(fine_latent)  # [B, 256, 4, 4]
        
        # Flatten and quantize
        B, C, H, W = coarse_feat.shape
        coarse_feat = coarse_feat.permute(0, 2, 3, 1).reshape(B, H*W, C)  # [B, 16, 256]
        
        coarse_logits = self.quantizer(coarse_feat)  # [B, 16, 1024]
        coarse_tokens = torch.argmax(coarse_logits, dim=-1)  # [B, 16]
        
        return coarse_tokens, coarse_logits

class FineResidualHead(nn.Module):
    """HART-inspired fine residual processing"""
    
    def __init__(self, in_channels=32, hidden_dim=128):
        super().__init__()
        
        # Lightweight UNet for residual prediction
        self.input_proj = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)
        
        # Down-sample
        self.down1 = nn.Sequential(
            nn.Conv2d(hidden_dim, hidden_dim*2, 3, stride=2, padding=1),
            nn.GroupNorm(8, hidden_dim*2),
            nn.SiLU()
        )
        
        # Mid processing
        self.mid = nn.Sequential(
            nn.Conv2d(hidden_dim*2, hidden_dim*2, 3, padding=1),
            nn.GroupNorm(8, hidden_dim*2),
            nn.SiLU(),
            nn.Conv2d(hidden_dim*2, hidden_dim*2, 3, padding=1),
        )
        
        # Up-sample
        self.up1 = nn.Sequential(
            nn.ConvTranspose2d(hidden_dim*2, hidden_dim, 3, stride=2, padding=1, output_padding=1),
            nn.GroupNorm(8, hidden_dim),
            nn.SiLU()
        )
        
        # Output projection
        self.output_proj = nn.Conv2d(hidden_dim, in_channels, 3, padding=1)
        
    def forward(self, x):
        """é¢„æµ‹æ®‹å·® - input: [B, 256, 32]"""
        # Reshape to spatial
        B, L, C = x.shape
        H = W = int(L ** 0.5)  # 16
        x = x.view(B, C, H, W)  # [B, 32, 16, 16]
        
        # UNet processing
        h = self.input_proj(x)       # [B, 128, 16, 16]
        h_down = self.down1(h)       # [B, 256, 8, 8]
        h_mid = self.mid(h_down)     # [B, 256, 8, 8]
        h_up = self.up1(h_mid)       # [B, 128, 16, 16]
        residual = self.output_proj(h_up)  # [B, 32, 16, 16]
        
        # Back to token format
        B, C, H, W = residual.shape
        return residual.view(B, H*W, C)  # [B, 256, 32]

class LiteHybridH100(nn.Module):
    """Lite-Hybrid for H100 - 256x256 images"""
    
    def __init__(self):
        super().__init__()
        
        # Base encoder (LiteVAE style)
        self.vae_encoder = LiteVAEEncoder()
        
        # Dual branches (HART inspiration)
        self.coarse_tokenizer = CoarseTokenizer()
        self.fine_residual_head = FineResidualHead()
        
        # Fusion layer
        self.fusion = nn.Sequential(
            nn.Linear(32, 64),  # Expand for fusion
            nn.SiLU(),
            nn.Linear(64, 32),  # Back to original dim
            nn.LayerNorm(32)
        )
        
        print(f"ğŸ”¥ Lite-Hybrid H100 Modelåˆå§‹åŒ–å®Œæˆ")
        total_params = sum(p.numel() for p in self.parameters()) / 1e6
        print(f"   æ€»å‚æ•°: {total_params:.1f}M")
        print(f"   è®¾å¤‡: {next(self.parameters()).device}")
        
    def encode_hybrid(self, images):
        """æ··åˆç¼–ç  - coarse + fine"""
        # Base VAE encoding
        base_tokens = self.vae_encoder.encode(images)  # [B, 256, 32]
        
        # Coarse branch
        coarse_tokens, coarse_logits = self.coarse_tokenizer(base_tokens)
        
        # Fine branch
        fine_residual = self.fine_residual_head(base_tokens)
        
        return {
            'coarse_tokens': coarse_tokens,
            'coarse_logits': coarse_logits,
            'fine_residual': fine_residual,
            'base_tokens': base_tokens
        }
    
    def forward(self, images):
        """å‰å‘ä¼ æ’­ - å®Œæ•´dual-branchå¤„ç†"""
        encoded = self.encode_hybrid(images)
        
        # Fusion: base + residual
        enhanced_tokens = encoded['base_tokens'] + encoded['fine_residual']
        enhanced_tokens = self.fusion(enhanced_tokens)
        
        return {
            'tokens': enhanced_tokens,
            'coarse_info': encoded['coarse_tokens'], 
            'fine_residual': encoded['fine_residual']
        }

def benchmark_h100_performance():
    """H100æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    print("ğŸ”¬ Lite-Hybrid H100æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    hybrid_model = LiteHybridH100().to(device).eval()
    baseline_model = LiteVAEEncoder().to(device).eval()
    
    # æµ‹è¯•æ•°æ® - 256x256 imagesé€‚åˆH100
    batch_sizes = [1, 4, 8] if device.type == 'cuda' else [1, 2]
    
    results = {}
    
    for batch_size in batch_sizes:
        print(f"\nğŸ“Š Batch Size {batch_size} æµ‹è¯•:")
        test_images = torch.randn(batch_size, 3, 256, 256).to(device)
        
        # FP16 for H100 efficiency
        if device.type == 'cuda':
            test_images = test_images.half()
            hybrid_model = hybrid_model.half()
            baseline_model = baseline_model.half()
        
        # Baselineæµ‹è¯•
        baseline_times = []
        for _ in range(10):
            if device.type == 'cuda':
                torch.cuda.synchronize()
            start = time.time()
            
            with torch.no_grad():
                _ = baseline_model.encode(test_images)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end = time.time()
            baseline_times.append((end - start) * 1000)
        
        baseline_avg = sum(baseline_times) / len(baseline_times)
        baseline_per_image = baseline_avg / batch_size
        
        # Hybridæµ‹è¯•
        hybrid_times = []
        for _ in range(10):
            if device.type == 'cuda':
                torch.cuda.synchronize()
            start = time.time()
            
            with torch.no_grad():
                _ = hybrid_model(test_images)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end = time.time()
            hybrid_times.append((end - start) * 1000)
        
        hybrid_avg = sum(hybrid_times) / len(hybrid_times)
        hybrid_per_image = hybrid_avg / batch_size
        
        # ç»“æœåˆ†æ
        latency_increase = hybrid_per_image - baseline_per_image
        
        print(f"  åŸºçº¿å»¶è¿Ÿ: {baseline_per_image:.2f}ms/å›¾")
        print(f"  Hybridå»¶è¿Ÿ: {hybrid_per_image:.2f}ms/å›¾")
        print(f"  å»¶è¿Ÿå¢åŠ : {latency_increase:.2f}ms ({(latency_increase/baseline_per_image)*100:.1f}%)")
        
        results[f'batch_{batch_size}'] = {
            'baseline_ms': baseline_per_image,
            'hybrid_ms': hybrid_per_image,
            'increase_ms': latency_increase,
            'increase_percent': (latency_increase/baseline_per_image)*100
        }
    
    # å†…å­˜ä½¿ç”¨
    if device.type == 'cuda':
        memory_used = torch.cuda.max_memory_allocated() / 1024**2
        print(f"\nğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")
        results['memory_mb'] = memory_used
    
    # ç›®æ ‡è¯„ä¼°
    best_batch = min(results.keys(), key=lambda k: results[k]['increase_ms'] if 'increase_ms' in results[k] else float('inf'))
    best_increase = results[best_batch]['increase_ms']
    target_achieved = best_increase <= 1.0  # â‰¤1msç›®æ ‡
    
    print(f"\nğŸ¯ ç›®æ ‡è¯„ä¼°:")
    print(f"  æœ€ä½³é…ç½®: {best_batch}")
    print(f"  æœ€å°å»¶è¿Ÿå¢åŠ : {best_increase:.2f}ms")
    print(f"  ç›®æ ‡è¾¾æˆ (â‰¤1ms): {'âœ…' if target_achieved else 'âŒ'}")
    
    if target_achieved:
        print("ğŸŸ¢ H100å®éªŒæˆåŠŸï¼å»¶è¿Ÿç›®æ ‡è¾¾æˆ")
        status = "SUCCESS"
    elif best_increase <= 2.0:
        print("ğŸŸ¡ æ¥è¿‘ç›®æ ‡ï¼Œè¡¨ç°è‰¯å¥½") 
        status = "PROMISING"
    else:
        print("ğŸ”´ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        status = "NEEDS_WORK"
    
    results['summary'] = {
        'best_batch': best_batch,
        'best_increase_ms': best_increase,
        'target_achieved': target_achieved,
        'status': status,
        'device': str(device)
    }
    
    return results

def validate_architecture():
    """æ¶æ„éªŒè¯ - ç¡®ä¿dual-branchæ­£å¸¸å·¥ä½œ"""
    
    print("\nğŸ” æ¶æ„éªŒè¯æµ‹è¯•")
    print("=" * 40)
    
    model = LiteHybridH100().to(device).eval()
    test_images = torch.randn(2, 3, 256, 256).to(device)
    
    if device.type == 'cuda':
        test_images = test_images.half()
        model = model.half()
    
    with torch.no_grad():
        # æµ‹è¯•ç¼–ç 
        encoded = model.encode_hybrid(test_images)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        result = model(test_images)
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {test_images.shape}")
    print(f"âœ… Coarse tokens: {encoded['coarse_tokens'].shape}")
    print(f"âœ… Fine residual: {encoded['fine_residual'].shape}")
    print(f"âœ… Final tokens: {result['tokens'].shape}")
    print(f"âœ… Coarse token range: [{encoded['coarse_tokens'].min().item()}, {encoded['coarse_tokens'].max().item()}]")
    
    return {
        'input_shape': list(test_images.shape),
        'coarse_tokens_shape': list(encoded['coarse_tokens'].shape),
        'fine_residual_shape': list(encoded['fine_residual'].shape),
        'final_tokens_shape': list(result['tokens'].shape),
        'validation_passed': True
    }

def main():
    """ä¸»å®éªŒå‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹Lite-Hybrid H100å®éªŒ (#6)")
    print("ğŸ¯ ç›®æ ‡: HARTçµæ„ŸåŒåˆ†æ”¯æ¶æ„ï¼Œå»¶è¿Ÿ+1msä»¥å†…")
    print("ğŸ­ å¹³å°: H100 80GB HBM3")
    print("=" * 70)
    
    try:
        # æ¶æ„éªŒè¯
        arch_results = validate_architecture()
        
        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        perf_results = benchmark_h100_performance()
        
        # åˆå¹¶ç»“æœ
        final_results = {
            'experiment': 'lite_hybrid_hart_h100',
            'timestamp': time.strftime('%Y%m%d_%H%M%S'),
            'architecture_validation': arch_results,
            'performance_benchmark': perf_results
        }
        
        # ä¿å­˜ç»“æœ
        results_file = f'lite_hybrid_h100_results_{final_results["timestamp"]}.json'
        with open(results_file, 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        print(f"\nğŸ‰ Lite-Hybrid H100å®éªŒå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜: {results_file}")
        
        summary = perf_results['summary']
        print(f"\nğŸ“Š å®éªŒæ€»ç»“:")
        print(f"  æ¶æ„éªŒè¯: {'âœ… é€šè¿‡' if arch_results['validation_passed'] else 'âŒ å¤±è´¥'}")
        print(f"  æ€§èƒ½çŠ¶æ€: {summary['status']}")
        print(f"  æœ€ä½³å»¶è¿Ÿå¢åŠ : {summary['best_increase_ms']:.2f}ms")
        print(f"  ç›®æ ‡è¾¾æˆ: {'âœ…' if summary['target_achieved'] else 'âŒ'}")
        print(f"  è¿è¡Œå¹³å°: {summary['device']}")
        
        if summary['status'] == 'SUCCESS':
            print("\nğŸ† å®éªŒå¤§æˆåŠŸï¼HARTçµæ„Ÿçš„åŒåˆ†æ”¯æ¶æ„åœ¨H100ä¸Šè¡¨ç°ä¼˜å¼‚")
            print("ğŸ“‹ ä¸‹ä¸€æ­¥: é›†æˆåˆ°å®Œæ•´ParScale-EAR pipeline")
        else:
            print(f"\nâš¡ çŠ¶æ€: {summary['status']} - æ¶æ„å¯è¡Œä½†éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        return final_results
        
    except Exception as e:
        print(f"âŒ H100å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()